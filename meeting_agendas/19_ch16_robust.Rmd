---
title: "ROBUST BAYESIAN METHODS"
subtitle: "Chapter 16"
author: "AG Schissler"
date: "7 Apr 2021 (*updated: `r Sys.Date()`)*"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(printr)
library(Bolstad)
## xaringan::inf_mr('./19_ch16_robust.Rmd')
```

# i. Admin & Startup

## Week 11 Focus

- Receive and act on HW 7 Ch. 11-13, HW 8 Ch.14 feedback (in progress)
- Assess progress via Midterm 3 canvas quiz, due Friday Apr ~~9~~ 16.
- Practice on HW 9 Ch.15, 16 content.
- Read Ch. 15, 16
- HW 9. Ch.15, 16 due Friday Apr 9.

## Today's plan

- Prime by discussion via HW 9, Ch.16 exercise prompts.
- Discuss Ch. 15
- Summarize, Revisit HW or Q & A

*Please contribute questions and comments*

# I. Preview HW 9

- Ch.15: 4, C1.
- Ch.16: 2, C1, C3 

# II. Discuss Ch. 16 with examples

- Overview 
- 16.1 Effect of a misspecified prior
- Example 16.1, 16.2
- 16.2 Bayes' Theorem with Mixture Priors
- Example 16.2 (con'd)
- Summary

## Ch.16 Overview

- One problem with Bayesian statistics is the unwillingness to formalize their prior knowledge, despite the all the benefits of Bayesian methods we have seen.
- Or the prior could be wrong or inconsisent with the data.
- This could have been caused by a misspeciﬁed prior that arose when the scientist based his/her prior on past data, which had been generated by a process that differs from the process that will generate the new data in some important way that the scientist failed to take into consideration. 
- In this chapter, we will learn a way to make Bayesian inference *robust* against prior misspecification.

# 16.1 Effect of a misspecified prior

- If the prior places high probability on values that have low likelihood, and low probability on values that have high likelihood, the posterior will place high probability on values that are not supported either by the prior or by the likelihood.
- This results in a posterior that is neither like the prior or the likelihood (a weighted average of the two functions).
- This is not satisfactory. 

## Example 16.1

- Discuss prompt on p.338.
- Discuss Figure 16.1.

## Example 16.2

- Discuss prompt on p.339.
- Discuss Figure 16.2.

# 16.2 Bayes' Theorem with Mixture Priors

* Using mixture priors protects against this possible misspeciﬁcation of the prior.
* We use mixtures of conjugate priors. 
* We do this by introducing a *mixture index random variable* that takes on the values 0 or 1.
* Represent multiple posteriors resulting from different priors by indexing using $i$: 

$g_i(\theta|y) \propto g_i(\theta) f(y|\theta)$

## The Mixture Prior

* The joint prior distribution of $\theta$ and $I$ is $g(\theta, i) = p_i \times (1-i) \times g_0 ( \theta ) + (1 - p_i) \times i \times g_1 ( \theta ), i=0,1$.
* Marginalizing over (summing) the two discrete values of $I$ gives:
* The marginal mixture prior is $g(\theta) = p_0 \times g_0 ( \theta ) + p_1 \times g_1 ( \theta )$ , where $g_0 ( \theta )$ is the original prior we believe in.
* And $g_1$ is another prior that has heavier tails.
* Allows for our original prior being wrong.
* We think our prior is correct so choose a high value, e.g., $p_0 = 0.95$.

## The Joint Posterior

* The respective posteriors that arise using each of the priors are
* $g_0 ( \theta | y_1 ,\ldots, y_n )$ and $g_1 ( \theta | y_1 ,\ldots , y_n )$.
* Discuss derivation on p.341-342 of the marginal posterior probability of $P( I = i | y)$.
- The mixture posterior is a mixture of the two posteriors, where the mixing proportions $P( I = i )$ for $i = 0 , 1$, are proportional to the prior probability times the the marginal (posterior)  probability (or probability density) evaluated at the data that occurred. 

$P( I = i | y) \propto p_i \times f_i ( y_1 , \ldots, y_n )$ for $i = 0 ,1$.

- They sum to 1, so 

$P(I=i | y) = \frac{p_i \times f_i(y)}{\sum_{i=0}^1 p_i \times f_i(y)}$ for $i=0,1$.


## The Mixture Posterior

- Bayes’ theorem is used on the mixture prior to determine a mixture posterior. 
- The mixture index variable is a *nuisance parameter* and is marginalized out.
- If the likelihood has most of its value far from the original prior, the mixture posterior will be close to the likelihood.
- This is a much more satisfactory result. 
- When the prior and likelihood are conﬂicting, we should base our posterior belief mostly on the likelihood, because it is based on the data. 
- Our prior was based on faulty reasoning from past data that failed to note some important change in the process we are drawing the data from. 

## The Mixture Posterior

- Marginalize over the possible index values $i$ to obtain the marginal posterior probability distribution $g(\theta|y) = \sum_{i=0}^1 g(\theta, i|y)$.
- Discuss derivation on p.343.
- Leading to Equation (16.2).

## Example 16.2 (con'd)

- Mixture prior for binomial $\pi$.
- Compare Figures 16.1 and 16.5.
- Mixture for normal mean
- Inspect Figures 16.6-16.8.

# Closing (5 min)

- Summary
- HW discussion
- other?
