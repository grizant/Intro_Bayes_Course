---
title: "Single-level Regression Part 2 and Simulating from Models"
author: "Presented by AG Schissler"
date: "04/20/2020"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE, cache.comments=TRUE)
suppressMessages(library(ggplot2))
suppressMessages(library(arm))
suppressMessages(library(R2OpenBUGS))
suppressMessages(library(foreign))
```

## Start of class work: Simulation

Design a simulation to answer the question: *how many girls in 400 births*? Assume the probability that a baby is a girl or boy is 48.8% and 51.2%, respectively. Suppose that 400 babies are born in a hosiptal in a given year. How many will be girs?

Now extend your simulation to get a sense of the *distribution* of what could happen. The aim of this experiment is to capture (quantify) the *uncertainty*.

```{r twins}
## YOUR CODE HERE
n.girls <- rbinom (1, 400, .488)
print (n.girls)

n.sims <- 1000
n.girls <- rep (NA, n.sims)
for (s in 1:n.sims){
    n.girls[s] <- rbinom (1, 400, .488)
}
hist (n.girls, main="")
```

## Today's lesson is derived from Gelman and Hill Ch. 4, 7

Most of these notes are direct quotes, paraphrases, and adaptations from 

[Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill 2007)](http://www.stat.columbia.edu/~gelman/arm/)

Your exercises will also be derived from the same textbook. You can find data sets and examples here:

http://www.stat.columbia.edu/~gelman/arm/software/

### The `arm` R package

I recommend you install the Applied Regression Modeling R package: `arm`. It has helpful convenience functions for fitting Bayesian (multilevel) regression modeling and simulating from these models.

```{r arm, include = F}
## From CRAN
## install.packages("arm")
```

## A note about the coding practices for the examples

The examples below frequently attach objects to the `R` global environment and to the `R2BUGS` search path. This works okay in isolation, but can be dangerous when objects have the same name and other scoping issues. I recommend to keep data in a data structure (`data.frame`, `matrix`, etc). And reference that object (instead of extracting vectors to use separate). It keeps things tidy and is safer. Also `R2BUGS`interactively requests confirmation for `attach.all` which is a pain when knitting your `rmarkdown` (not sure if RStudio handles this automatically, but you can try this from the command line `R -e "rmarkdown::render('23_SingleLevel_Reg2_SimModelsInf_20apr2020.Rmd')"` to run noninteratively). Also many values are *hardcoded* instead of done in a dynamic way. Moreover, it is more natural in `R` to write functions for repeated tasks and avoid loops (and use the `apply` family of functions). Please keep that in mind as you reuse the code in your assignments and projects (and future work).

## Regression diagnostics

Try the `lindia` package

```{r lindia, eval=FALSE}
## install.packages("lindia")
library(lindia)
library(MASS)
data(Cars93)
# a regression with categorical variable
cars_lm <- lm(Price ~ Passengers + Length + RPM + Origin, data = Cars93)
gg_diagnose(cars_lm)
# customize which diagnostic plot is included
plots <- gg_diagnose(cars_lm, plot.all = FALSE)
names(plots)     # get name of the plots
exclude_plots <- plots[-c(1, 3) ]    #exclude certain diagnostics plots
include_plots <- plots[c(1, 3)]      # include certain diagnostics plots
plot_all(exclude_plots)              # make use of plot_all() in lindia
plot_all(include_plots)
```

## Ch.4 Linear regression: before and after fitting the model

It is not always appropriate to fit a classical (single-level) linear regression model using data in their raw form. As we discuss in Section 4.1 and 4.4, linear and logarithmic transformations can sometimes help the interpretation of the model. Nonlinear transformations of the data are sometimes necessary to more closely satisfy additivity and linearity assumptions, which in turn should improve the fit and predictive power of the model. Section s4.5 presents some other univariate transformations that are occasionally useful.

### 4.1 Linear transformations

Linear transformations **do not affect** the fit of a classical regression model, and they do not affect predictions: the changes in the inputs and the coefficients cancel in forming the predicted value $X \beta$. However, well-chosen linear transformations can improve interpretations of coefficients and make a fitted model easier to understand. And they may improve the **accuracy** of your estimation which increases statistical power when testing coefficients.

#### Untransformed (raw) data

```{r sect41}
## make sure to download the data first and specify your path
heights <- read.dta ("data/heights.dta")
suppressMessages( attach.all (heights))

## str(heights)

## recode sex variable
male <- 2 - sex

## (for simplicity) remove cases with missing data
ok <- !is.na (earn+height+sex) & earn>0
heights.clean <- as.data.frame (cbind (earn, height, male)[ok,])
n <- nrow (heights.clean)
suppressMessages(attach.all (heights.clean))
height.jitter.add <- runif (n, -.2, .2)

## Model fit
lm.earn <- lm (earn ~ height)
display (lm.earn)
sim.earn <- sim (lm.earn)
beta.hat <- coef(lm.earn)

## Figure 4.1 (left)
par (mar=c(6,6,4,2)+.1)
plot (height + height.jitter.add, earn, xlab="height", ylab="earnings", pch=20, mgp=c(4,2,0), yaxt="n", col="gray10",
      main="Fitted linear model")
axis (2, c(0,100000,200000), c("0","100000","200000"), mgp=c(4,1.1,0))
for (i in 1:20){
    curve (sim.earn@coef[i,1] + sim.earn@coef[i,2]*x, lwd=.5, col="gray", add=TRUE)}
curve (beta.hat[1] + beta.hat[2]*x, add=TRUE, col="red")

## Figure 4.1 (right) 
par (mar=c(6,6,4,2)+.1)
plot (height + height.jitter.add, earn, xlab="height", ylab="earnings", pch=20, mgp=c(4,2,0), yaxt="n", col="gray10",
      main="Fitted linear model",xlim=c(0,80),ylim=c(-200000,200000))
axis (2, c(-100000,0,100000), c("-100000","0","100000"), mgp=c(4,1.1,0))
for (i in 1:20){
    curve (sim.earn@coef[i,1] + sim.earn@coef[i,2]*x, lwd=.5, col="gray", add=TRUE)}
curve (beta.hat[1] + beta.hat[2]*x, add=TRUE, col="red")
```

#### Scaling of predictors and regression coefficients

Ignoring the adequacy of fit for now, we interpret the slope coefficient as an exected $1300 increase for an inch difference in height. This seems like a lot, but a minimeter scale we get for the same data a $51 increase. Or on the mile-scale, a $81,000,000 increase. This creates problems interpreting and can make models very misleading. 

We need some sense of variation to interpret data on a comparable scale. One option is to *standardize* by scaling the heights by their sample standard deviation 3.8:

```{r sect41b}
heights.clean$height2 <- heights.clean$height / sd(heights.clean$height)
lm.earn <- lm(earn ~ height2, data = heights.clean)
display(lm.earn)
```

Now for each standard deviation increase in height (about 4 inches), we expect around $4900 increase in earnings.

#### Standardization by z-scores

One can go a bit further and also **center** the data by substracting the sample mean prior to scaling by sd.


```{r sect41c}
heights.clean$heightZ <- ( heights.clean$height - mean(heights.clean$height) ) / sd(heights.clean$height)
## or use scale()
mean(heights.clean$heightZ); sd(heights.clean$heightZ)
lm.earn <- lm(earn ~ heightZ, data = heights.clean)
display(lm.earn)
```

Now we have the same interpretation as before except the intercept now corresponds to the average $y$ value when all predictors are at their mean values. 

#### Scaling using reasonable scales

You can scale by any reasonable quantity to aid in interpretation.

### 4.2 Centering and standardizing, especially for models with interactions

We now explore centering/standardizing in a regression predicting a child's IQ from maternal characteristics.

### original model

```{r sect42}
kidiq <- read.dta("data/kidiq.dta")
suppressMessages(attach(kidiq))

## Estimations
fit.4 <- lm (kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq)
display(fit.4)
```

Interpreting these coefficients is difficult. Does the 51.3 mean that mother's who complete HS do, on average, 51.3 points better? No! The model includes an interaction, and 51.3 is the predicted difference for kids that differ in `mom.hs`, *among* the children who mothers `mom.iq` = 0. Since no `mom.iq` is 0, this value has little meaning.

#### centering by subtracting the mean

```{r sect42b}
c_mom_hs <- mom_hs - mean(mom_hs)
c_mom_iq <- mom_iq - mean(mom_iq)

fit.5 <- lm (kid_score ~ c_mom_hs + c_mom_iq + c_mom_hs:c_mom_iq)
display(fit.5)
```

The resulting regression is easier to interpret, with each main effect corresponding to a predictive difference with the other value at its **average**.

#### using a conventional centering point

Or we can take the midpoint of the range of predictor (see `mom_hs` below). Or a known population value (IQ of 100).

```{r sect42c}

c2_mom_hs <- mom_hs - 0.5
c2_mom_iq <- mom_iq - 100

fit.6 <- lm (kid_score ~ c2_mom_hs + c2_mom_iq + c2_mom_hs:c2_mom_iq)
display(fit.6)
```

#### centering by subtracting the mean & dividing by 2 sd

We still have an issue that the effect of `mom_iq` seems smaller than `mom_hs`. But does this reflect the reality? 1-point increase in IQ is not very much, but switching entire categories of `mom_hs` is a big deal. A natural solution would be scale by standard deviation. Here we'll scale by 2 sd (we divide by 2 standard deviations rather than 1 to maintain coherence when considering binary variables.). Now a 1-unit change in the rescaled predictor corresponds to a change of 1 standard deviation below the mean, to 1 standard devation above.

```{r sect42d}
z_mom_hs <- (mom_hs - mean(mom_hs))/(2* sd(mom_hs))
z_mom_iq <- (mom_iq - mean(mom_iq))/(2* sd(mom_iq))

fit.7 <- lm (kid_score ~ z_mom_hs + z_mom_iq + z_mom_hs:z_mom_iq)
display(fit.7)
```

We can now interpret all coefficients on a common scale (except for the intercept). In regression it is usually fine to just scale by 1 sd but just take care comparing values directly.

### 4.4 Logarithm transformations

When additivity and linearity are not reasonable assumptions, a nonlinear transformation can sometimes remedy the situation. Logarithm is great choice to linearize, as multiplicative effects become additive on the log scale. And logarithmic transformations enforce positivity of the values and errors.

#### Height and earning example

We illustrate logarithmic regression by considering modeling predicting earnings from height.

#### Direct interpretation of small coefficients on the log scale

```{r sect44}
## Log transformation
rm(male)
suppressMessages(attach.all(heights.clean))

log.earn <- log(earn)
earn.logmodel.1 <- lm(log.earn ~ height)
display(earn.logmodel.1)
 ```
 
The estimated coefficient $\beta_1 = 0.06$ implies a 1-inch difference in heights corresponds to 6% increase in earnings, since $exp(0.06) \approx 1.06$. But for larger effects this approximation is poor.
 
 ```{r sect44b}
## Figure 4.3
sim.logmodel.1 <- sim (earn.logmodel.1)
beta.hat <- coef (earn.logmodel.1)

n <- nrow(heights.clean)

par (mar=c(6,6,4,2)+.1)
plot (height + runif(n,-.2,.2), log.earn, xlab="height", ylab="log (earnings)", pch=20, yaxt="n", mgp=c(4,2,0), col="gray10",
      main="Log regression, plotted on log scale")
axis (2, seq(6,12,2), mgp=c(4,1.1,0))
for (i in 1:20)
  curve (sim.logmodel.1@coef[i,1] + sim.logmodel.1@coef[i,2]*x, lwd=.5, col="gray", add=TRUE)
curve (beta.hat[1] + beta.hat[2]*x, add=TRUE, col="red")

par (mar=c(6,6,4,2)+.1)
plot (height + runif(n,-.2,.2), earn, xlab="height", ylab="earnings", pch=20, yaxt="n", mgp=c(4,2,0), col="gray10",
      main="Log regression, plotted on original scale")
axis (2, c(0,100000,200000), c("0","100000","200000"), mgp=c(4,1.1,0))
for (i in 1:20)
  curve (exp(sim.logmodel.1@coef[i,1] + sim.logmodel.1@coef[i,2]*x), lwd=.5, col="gray", add=TRUE)
curve (exp(beta.hat[1] + beta.hat[2]*x), add=TRUE, col="red")
```

6% increase seems like a lot? But what if it is just the fact that men are taller on average?

#### Log scale regression model
```{r sect44c}

earn.logmodel.2 <- lm(log.earn ~ height + male)
display(earn.logmodel.2)

## Including interactions

earn.logmodel.3 <- lm(log.earn ~ height + male + height:male)
display(earn.logmodel.3)

## Linear transformations
z.height <- (height - mean(height))/sd(height)
earn.logmodel.4 <- lm(log.earn ~ z.height + male + z.height:male)
display(earn.logmodel.4)
```

#### Log-log model

```{r sect44d}
log.height <- log(height)
earn.logmodel.5 <- lm(log.earn ~ log.height + male)
display(earn.logmodel.5)
```

Now for each 1% difference in heights, we predict a 1.41% increase in earnings.

### 4.5 Other transformations

#### Square root

Square root can be considered, but lacks the clean interpretation the other methods. But it is useful when the log is too severe.

#### Idiosyncratic transformations

Sometimes it is useful to develop tailored transformations for specific problems. For example, how to handle logarithmic transformations of variables that include 0?

#### Using continuous rather than discrete

I prefer to usually leave continuous values as continuous rather than binning into categories. It is more natural, can be more statistically powerful in testing coefficients, and retains the underlying information.

That being said, it sometimes interesting to create categories. For example, bin US ages into generations. This may better reflect cultural differences than an average prediction for 1 year increase in age.

### 4.6 Building regression models for prediction

#### General principles

1. Include all variables that, for substantive reasons, might be expected to be important in predicting the outcome.
2. It is not always necessary to include these inputs as separate predictors ---- for example create a composite average or total "score".
3. For inputs that have large effects, consider including their interactions as well.
4. We suggest the following strategy for decisions whether to exclude a variable from a prediction model based on expected sign and statistical significance.
   * If the predictor is **not** significance, but has the expected sign, it is fine to keep it in.
   * If the predictor is **not** significance, but does **not** have the expected sign, it may help to remove the variable (decrease noise).
   * If the predictor **is** significance, but does **not** have the expected sign. There is something strange going on. Consider lurking variables and any other explanations. Possibly find/measure confounders and include them in the model.
   * If the predictor is significance, but has the expected sign, include the term.

### 4.7 Fitting a series of regressions

It is common to fit a regression model repeatedly, either on different data or to subsets of the data.

#### Predicting US political party identification

We will illustrate this idea by fitting a series of cross-sectional regressions to learn trends over time for US party identification. The National Election Survey asks how strongly a person identifying with a political party (1 = strong Democrat, 2 = Democrat, 3 = weak Dem, 4 = independent, ..., 7 = strong Republican). We'll treat this as a continuous variable.

```{r sect47, fig.cap = "Estimated coefficients (and 50% intervals) for regression of party identification on political ideology, self-reported race, and others, as fit separately to poll data form 1976 to 2000. The plots are on different scales, with input variables ordered roughly in declining order of the magnitude of their estimated coefficients."}
brdata <- read.dta("data/nes5200_processed_voters_realideo.dta",convert.factors=F)
## Data are at http://www.stat.columbia.edu/~gelman/arm/examples/nes

## Clean the data
brdata <- brdata[is.na(brdata$black)==FALSE&is.na(brdata$female)==FALSE&is.na(brdata$educ1)==FALSE
                 &is.na(brdata$age)==FALSE&is.na(brdata$income)==FALSE&is.na(brdata$state)==FALSE,]
kept.cases <- 1952:2000
matched.cases <- match(brdata$year, kept.cases)
keep <- !is.na(matched.cases)
data <- brdata[keep,]
plotyear <- unique(sort(data$year))
year.new <- match(data$year,unique(data$year))
n.year <- length(unique(data$year))
income.new <-data$income-3
age.new <- (data$age-mean(data$age))/10
y <- data$rep_pres_intent
data <- cbind(data, year.new, income.new, age.new, y)
nes.year <- data[,"year"]
age.discrete <- as.numeric (cut (data[,"age"], c(0,29.5, 44.5, 64.5, 200)))
race.adj <- ifelse (data[,"race"]>=3, 1.5, data[,"race"])
data <- cbind (data, age.discrete, race.adj)

female <- data[,"gender"] - 1
black <- ifelse (data[,"race"]==2, 1, 0)
rvote <- ifelse (data[,"presvote"]==1, 0, ifelse(data[,"presvote"]==2, 1, NA))

region.codes <- c(3,4,4,3,4,4,1,1,5,3,3,4,4,2,2,2,2,3,3,1,1,1,2,2,3,2,4,2,4,1,1,4,1,3,2,2,3,4,1,
                  1,3,2,3,3,4,1,3,4,1,2,4)
suppressMessages(attach.all(data))

## Regression & plot
regress.year <- function (yr) {
    this.year <- data[nes.year==yr,]
    lm.0 <- lm (partyid7 ~ real_ideo + race.adj + factor(age.discrete) + educ1 + gender + income,
                data=this.year)
    coefs <- summary(lm.0)$coef[,1:2]
}

summary <- array (NA, c(9,2,8))
for (yr in seq(1972,2000,4)){
    i <- (yr-1968)/4
    summary[,,i] <- regress.year(yr)
}
yrs <- seq(1972,2000,4)

coef.names <- c("Intercept", "Ideology", "Black", "Age.30.44", "Age.45.64", "Age.65.up", 
                "Education", "Female", "Income")

par (mfrow=c(2,5), mar=c(3,4,2,0))
for (k in 1:9){
    plot (range(yrs), range(0,summary[k,1,]+.67*summary[k,2,],summary[k,1,]-.67*summary[k,2,]), 
          type="n", xlab="year", ylab="Coefficient", main=coef.names[k], mgp=c(1.2,.2,0), cex.main=1,
          cex.axis=1, cex.lab=1, tcl=-.1)
    abline (0,0,lwd=.5, lty=2)
    points (yrs, summary[k,1,], pch=20, cex=.5)
    segments (yrs, summary[k,1,]-.67*summary[k,2,], yrs, summary[k,1,]+.67*summary[k,2,], lwd=.5)
}
```

## Ch. 7 Simulation of probability models and statistical inferences

Let's complicate the model from the beginning of class. Let's further assume there is a 1/125 chance that a birth results in fraternal twins and of which there is an approximate 49.5% chance of the twin being a girl, and a 1/300 chance of identical twins, which have an approximate 49.5% chance of being girls. We simulate 400 births as follows:

```{r ch7twins}

## Accounting for twins
birth.type <- sample (c("fraternal twin", "identical twin", "single birth"),
                      size=400, replace=TRUE, prob=c(1/25, 1/300, 1 - 1/25 - 1/300))
girls <- rep (NA, 400)
for (i in 1:400){
    if (birth.type[i]=="single birth"){
        girls[i] <- rbinom (1, 1, .488)}
    else if (birth.type[i]=="identical twin"){
            girls[i] <- 2*rbinom (1, 1, .495)}
    else if (birth.type[i]=="fraternal twin"){
                girls[i] <- rbinom (1, 2, .495)}
}
n.girls <- sum (girls)
print(n.girls)
```

```{r ch7twinsb}
## putting in a loop

n.sims <- 1000
n.girls <- rep (NA, n.sims)
for (s in 1:n.sims){
    birth.type <- sample (c("fraternal twin", "identical twin", "single birth"),
                          size=400, replace=TRUE, prob=c(1/25, 1/300, 1 - 1/25 - 1/300))
    girls <- rep (NA, 400)
    for (i in 1:400){
        if (birth.type[i]=="single birth"){
            girls[i] <- rbinom (1, 1, .488)}
        else if (birth.type[i]=="identical twin"){
                girls[i] <- 2*rbinom (1, 1, .495)}
        else if (birth.type[i]=="fraternal twin"){
                    girls[i] <- rbinom (1, 2, .495)}
    }
    n.girls[s] <- sum (girls)
}

par(mfrow = c(1,1))
hist(n.girls)
mean(n.girls); sd(n.girls)

## or more compactly
## girls <- ifelse (birth.type=="single birth", rbinom (400, 1, .488),
##                  ifelse (birth.type=="identical twin", 2*rbinom (400, 1, .495),
##                         rbinom (400, 2, .495)))
```

### 7.1 Simulation of probability models

#### An example of continuous predictive simulations

Now let's practice simulating continuous random variables. Suppose 52% of adults in the US are women and 48% are men. The heights of the men are approximately normally distributed with mean 69.1 incheds and standard deviation of 2.9 inches; women with a mean of 63.7 in. and sd 2.7 in. Let's simulate selecting 10 adults at random. What can we say about their average height?

```{r sect71}
woman <- rbinom (10, 1, .52)
height <- ifelse (woman==0, rnorm (10, 69.1, 2.9), rnorm (10, 64.5, 2.7))
avg.height <- mean (height)
print(avg.height)
```

To simulate the distribution of average heights:

```{r sect71b}
## simulation & Figure 7.1
n.sims <- 1000
avg.height <- rep (NA, n.sims)
for (s in 1:n.sims){
  sex <- rbinom (10, 1, .52)
  height <- ifelse (sex==0, rnorm (10, 69.1, 2.9), rnorm (10, 64.5, 2.7))
  avg.height[s] <- mean (height)
}
hist (avg.height, main="Average height of 10 adults") 
```

What about the maximum height?

```{r sect71c}
## simulation for the maximum height

n.sims <- 1000
max.height <- rep (NA, n.sims)
for (s in 1:n.sims){
  sex <- rbinom (10, 1, .52)
  height <- ifelse (sex==0, rnorm (10, 69.1, 2.9), rnorm (10, 64.5, 2.7))
  max.height[s] <- max (height)
}
hist (max.height, main="Maximum height of 10 adults")
```

To simulate using custom-made functions

```{r sect71d}
Height.sim <- function (n.adults){
  sex <- rbinom (n.adults, 1, .52)
  height <- ifelse (sex==0, rnorm (10, 69.1, 2.9), rnorm (10, 64.5, 2.7))
  return (mean(height))
}

avg.height <- replicate (1000, Height.sim (n.adults=10))
hist (avg.height, main="Average height of 10 adults")
```

### 7.2 Summarizing linear regression using simulation: an informal Bayesian approach

In a regression setting, we can use simulation to capture both predictive uncertainty and inferential (estimation) uncertainty.We first discuss the simplest case of simulating prediction errors, then consider inferential uncertainty and the **combination of both sources**.

#### Simulation to represent predictive uncertainty

We return to the `heights.dta` data and illustrate predictive uncertainty by considering the earnings of 68-inch-tall US male.

```{r sect72, include = FALSE}
rm(male, height)
suppressMessages(attach.all (heights.clean))

## Model of log earnings with interactions

earn.logmodel.3 <- lm (log.earn ~ height + male + height:male)
display (earn.logmodel.3)
```

```{r sect72b}
x.new <- data.frame (height=68, male=1)
pred.interval <- predict (earn.logmodel.3, x.new, interval="prediction", 
                          level=.95)
print (exp (pred.interval))
```

```{r sect72c}

## Constructing the predictive interval using simulation

pred <- exp (rnorm (1000, 9.95, .88))
pred.original.scale <- rnorm (1000, 9.95, .88)

## Histograms (Figure 7.2)

par (mfrow=c(1,2))
hist (pred.original.scale, xlab="log(earnings)", main="")
hist (pred, xlab="earnings", main="")
```

#### Why do we need simulation for predictive inferences?

```{r sect72d}

pred.man <- exp (rnorm (1000, 8.4 + 0.17*68 - 0.079*1 + .007*68*1, .88))
pred.woman <- exp (rnorm (1000, 8.4 + 0.17*68 - 0.079*0 + .007*68*0, .88))
pred.diff <- pred.man - pred.woman
pred.ratio <- pred.man/pred.woman
```

#### Simulation to represent uncertainty in regression coefficients

```{r 72e}
n.sims <- 1000
fit.1<- lm (log.earn ~ height + male + height:male)
sim.1 <- sim (fit.1, n.sims)

height.coef <- sim.1@coef[,2]
mean (height.coef)
sd (height.coef)
quantile (height.coef, c(.025, .975))

height.for.men.coef <- sim.1@coef[,2] + sim.1@coef[,4]
quantile (height.for.men.coef, c(.025, .975))

## Inside the sim function
## I admit the details to focus on the concept
## Note the use of multivariate normal
##for (s in 1: n.sims){
##  sigma[s] <- sigma.hat*sqrt((n-k)/rchisq (1, n-k))
##  beta[s] <- mvrnorm (1, beta.hat, V.beta*sigma[s]^2)
##}
##return (list (coef=beta, sigma=sigma))
```

### 7.3 Simulation for nonlinear predictions: congressional elections

#### Background and data issues

```{r sect73}
## Data are at http://www.stat.columbia.edu/~gelman/arm/examples/cong3

congress <- vector ("list", 49)
for (i in 1:49){
    year <- 1896 + 2*(i-1)
    file <- paste ("data/cong3/", year, ".asc", sep="")
    data.year <- matrix (scan (file, quiet = T), byrow=TRUE, ncol=5)
    data.year <- cbind (rep(year, nrow(data.year)), data.year)
    congress[[i]] <- data.year
}

## Note: download all ".asc" files into your R working directory in a file
## named cong3 for the above command to work

i86 <- (1986-1896)/2 + 1
cong86 <- congress[[i86]]
cong88 <- congress[[i86+1]]
cong90 <- congress[[i86+2]]

v86 <- cong86[,5]/(cong86[,5]+cong86[,6])
bad86 <- cong86[,5]==-9 | cong86[,6]==-9
v86[bad86] <- NA
contested86 <- v86>.1 & v86<.9
inc86 <- cong86[,4]

v88 <- cong88[,5]/(cong88[,5]+cong88[,6])
bad88 <- cong88[,5]==-9 | cong88[,6]==-9
v88[bad88] <- NA
contested88 <- v88>.1 & v88<.9
inc88 <- cong88[,4]

v90 <- cong90[,5]/(cong90[,5]+cong90[,6])
bad90 <- cong90[,5]==-9 | cong90[,6]==-9
v90[bad90] <- NA
contested90 <- v90>.1 & v90<.9
inc90 <- cong90[,4]

jitt <- function (x,delta) {x + runif(length(x), -delta, delta)}

## Plot Figure 7.3

v88.hist <- ifelse (v88<.1, .0001, ifelse (v88>.9, .9999, v88))
hist (v88.hist, breaks=seq(0,1,.05),
      xlab="Democratic share of the two-party vote", ylab="", yaxt="n",
      cex.axis=1.1, cex.lab=1.1, cex.main=1.2, 
      main="Congressional elections in 1988")
```

#### Fitting the model

```{r 73b}
v86.adjusted <- ifelse (v86<.1, .25, ifelse (v86>.9, .75, v86))
vote.86 <- v86.adjusted[contested88]
incumbency.88 <- inc88[contested88]
vote.88 <- v88[contested88]

fit.88 <- lm (vote.88 ~ vote.86 + incumbency.88)
display (fit.88)

## Figure 7.4

## 7.4 (a)

par (mfrow=c(1,1))
par (pty="s", mar=c(5,5,4,1)+.1)
plot (0, 0, xlim=c(0,1), ylim=c(0,1), type="n",
      xlab="Democratic vote share in 1986", ylab="Democratic vote share in 1988",
      cex.lab=1)
abline (0,1, lwd=.5)
j.v86 <- ifelse (contested86, v86, jitt (v86, .02))
j.v88 <- ifelse (contested88, v88, jitt (v88, .02))
points (j.v86[inc88==0], j.v88[inc88==0], pch=1)
points (j.v86[inc88==1], j.v88[inc88==1], pch=16)
points (j.v86[inc88==-1], j.v88[inc88==-1], pch=4)
mtext ("Raw data (jittered at 0 and 1)", line=1, cex=1.2)

## 7.4 (b)

par (pty="s", mar=c(5,5,4,1)+.1)
plot (0, 0, xlim=c(0,1), ylim=c(0,1), type="n",
      xlab="Democratic vote share in 1986", ylab="Democratic vote share in 1988",
      cex.lab=1)
abline (0,1, lwd=.5)
v86.adjusted <- ifelse (v86<.1, .25, ifelse (v86>.9, .75, v86))
vote.86 <- v86.adjusted[contested88]
vote.88 <- v88[contested88]
incumbency.88 <- inc88[contested88]
points (vote.86[incumbency.88==0], vote.88[incumbency.88==0], pch=1)
points (vote.86[incumbency.88==1], vote.88[incumbency.88==1], pch=16)
points (vote.86[incumbency.88==-1], vote.88[incumbency.88==-1], pch=4)
mtext ("Adjusted data (imputing 0's and 1's to .75)", line=1, cex=1.2)
```

#### Simulation for inferences and predictions of new data points

```{r 73c}
incumbency.90 <- inc90
vote.88 <- v88
n.tilde <- length (vote.88)
X.tilde <- cbind (rep (1, n.tilde), vote.88, incumbency.90)

n.sims <- 1000
sim.88 <- sim (fit.88, n.sims)
y.tilde <- array (NA, c(n.sims, n.tilde))
for (s in 1:n.sims){
    pred <- X.tilde %*% sim.88@coef[s,]
    ok <- !is.na(pred)
    y.tilde[s,ok] <- rnorm (sum(ok), pred[ok], sim.88@sigma[s])
}

## Predictive simulation for a nonlinear function of new data

y.tilde.new <- ifelse (y.tilde=="NaN", 0, y.tilde)
dems.tilde <- rowSums (y.tilde.new > .5)

## or
dems.tilde <- rep (NA, n.sims)
for (s in 1:n.sims){
    dems.tilde[s] <- sum (y.tilde.new[s,] > .5)
}
```

#### Implementation using functions

```{r 74d}
Pred.88 <- function (X.pred, lm.fit){
    sim.88 <- sim (lm.fit, 1)
    pred <- X.tilde %*% t(sim.88@coef)
    ok <- !is.na(pred)
    n.pred <- length (pred)
    y.pred <- rep (NA, n.pred)
    y.pred[ok] <- rnorm (sum(ok), pred[ok], sim.88@sigma)
    return (y.pred)
}

y.tilde <- replicate (1000, Pred.88 (X.tilde, fit.88))
dems.tilde <- replicate (1000, Pred.88 (X.tilde, fit.88) > .5)
```

## Closing

- What was an important point about the lesson?
- How will this lesson change your approach to regression modeling?




