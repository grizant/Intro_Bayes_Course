---
title: "Discrete random variables Ch. 5"
author: "AG Schissler"
date: "01/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

<bring to class: dice>

## i. Start-of-class work: Die rolling

Let's dig into an activity based on "Thought Experiment 1" (Section 5.1, p.84). Please roll your 10-sided 10 times and tabulate your results. Join with a partner or two and sketch a relative frequency histogram (example on p.85) of your combined results.

### Monte Carlo study: Long-run relative frequency converges to probability of each discrete value

Now let's write R code together simulate and visualize this process:

```{r, one}
num_rolls <- c(1e1, 1e2, 1e3, 1e4, 1e5, 1e6)
num_faces <- 10

## loop not efficient, but easy to explain

sim_data <- NULL
set.seed(11)
for (tmp_rolls in num_rolls){
    sim_faces <- sample(x = num_faces, size = tmp_rolls, replace = TRUE)
    sim_prop <- table(sim_faces) / tmp_rolls
    tmp_data <- data.frame(face = 1:10, prop = 0, num_rolls = tmp_rolls)
    ## careful inserting data if none occurred
    tmp_data$prop[tmp_data$face %in% names(sim_prop)] <- sim_prop
    sim_data <- rbind(sim_data, tmp_data)
}

## faceted plot, like Figure 5.1, p. 85

p0 <- ggplot(data = sim_data, aes(x = factor(face), y = prop))
p0 + geom_hline(yintercept = 0.1) + geom_bar(stat = "identity") + facet_wrap(~ num_rolls, nrow = 2) 

```

### Think-pair-share: HW 1 Exercise 4.10

1. Work Exercise 4.10 independently
2. Pair and discuss
3. Share with class

## 1. Probability distribution of a discrete rv (Section 5.1, 5.2)

A random variable describes the outcome of a random experiment in terms of a number. If the only possible outcomes of the experiment are distinct numbers separated from each other, we say the random variable is *discrete*. See Section 5.1 for more details.

### Expected value

Brief discussion: expected value as a weighted average.

### Variance

Discuss the impact of squaring deviations.

### Mean and variance of a linear transformation

Discuss derivations on p.89.

## 2. Three important discrete rvs (Sections 5.3 - 5.5)

Brief discussion of characterization/motivation of some common discrete rvs.

### Binomial distribution

Exercise HW 2 5.3

1. Work the exercises independently
2. Pair and discuss
3. Share with class

An R demonstration for 5.3:

```{r, fiveThree}
## our code here
```

### Hypergeometric

Exercise 5.6 a - c, d challenge

1. Work the exercises independently
2. Pair and discuss
3. Share with class

An R demonstration for 5.6:

```{r, fiveSix}
## all four draws are green
(20/50)*(19/49)*(18/48)*(17/47)

## GGGR
(20/50)*(19/49)*(18/48)*(30/47)
## GGRG
(20/50)*(19/49)*(30/48)*(18/47)

y <- 0:4
prob_y <- dhyper(x = y, m = 20, n = 30, k = 4)
```

### Poisson

Exercise 5.8

1. Work the exercises independently
2. Pair and discuss
3. Share with class

An R demonstration for 5.3:

```{r, fiveEight}
## our code here
```

## An R demonstration for Exercise 5.7:

```{r, fiveSeven}
dpois(x = 2, lambda = 2)
```

## 3. Joint random variables (Section 5.6)

Let's discuss notation and the tabular representation for discrete rvs (Table 5.3, p.97).

### Expectated value of a sum of two random variables

$E[X + Y] = E[X] + E[Y]$ for any two rvs.

### Variance of a sum of two random variables

Let's discuss the variance of a sum and a decomposition into a triple sum (p.98).

Introduce the notion of covariance.

### Special case: variance of a sum of independent rvs

Whole group discussion.

### Mean and variance of a difference of independent rvs

Simulate to show how summing OR substracting independent rvs increases variation according to equation 5.12.

```{r}
set.seed(22)
n <- 1e6
x <- rbinom(n = n, size = 10, prob = 0.2)
## mean of 2, 10*(.2)*(.8) = 1.6
y <- rpois(n = n, lambda = 2)

## estimate quantities
mean(x + y)
mean(x) + mean(y)
mean(x - y)
mean(x) - mean(y)

var(x + y) 
var(x) + var(y)
var(x - y) 
var(x) + var(y)
```

### Example 5.2

Let's discuss example 5.2.

## 4. Conditional probability for joint rvs.

Reduced universe idea again (Tables 5.4 and 5.5).

### Continue example 5.2

Whole class discussion with doc cam.

### Conditional probability through the multiplication rule

Recall that $f(y_j | x_i) = \frac{f(x_i, y_j)}{f(x_i)}$.  

which implies that $f(x_i, y_j) = f(x_i) \times f(y_j | x_i)$.

Remember that $X$ is an unobservable parameter while $Y$ is an observable rv (data) whose probability distribution depends on the parameter in Bayesian statistics. We'll develop Bayes' theorem using this rule for discrete rvs in Chapter 6.

## Closing

Please answer the following questions and be prepared to share with the group.

- What are the two most important properties of random variables we focused on computing?
- What was the main point from the Monte Carlo experiment?
- What was the muddiest point of today's meeting?
