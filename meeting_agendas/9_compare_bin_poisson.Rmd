---
title: "Comparing Inferences for Proportion (Ch.9) and Bayesian Inference for the Poisson mean (Ch.10)"
author: "AG Schissler"
date: "2/24/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Start-of-class work: Hands-on Monte Carlo data collection

We will conduct simulation to approximate the sampling distribution of Frequentist and Bayesian estimator of $\pi$.

Each table receives an index card with a value of $\pi \in \{0.1, 0.2, \ldots, 0.9\}$ and dice.

Student teams will

1. Design an experiment with your dice to perform random binomial experiments with modeling parameters $Y \sim bin(\pi = \pi_, n = 10)$.

2. **Each member** at your table completes an the random experiment and collects data, $y$.

## I. Quickly discuss the rest of Ch.9 that we didn't cover last time

Let's discuss Section 9.3, Figure 9.2 on projector.

### Interval estimation

#### Confidence intervals

### Comparing confidence and credible intervals

### Hypothesis testing 

### Testing a one-sided hypothesis

#### Frequentists tests of one-sided hypothesis

#### Bayesian tests of one-sided hypothesis

#### Aside: Numerical integration in R

```{r}
integrate(dbeta, 0, 0.15, 7, 75)
integrate(dbeta, 0.15, 1, 7, 75)
pbeta(0.15, 7, 75, lower.tail = FALSE)
```

### Testing a two-sided hypothesis

#### Frequentists tests of two-sided hypothesis

#### Bayesian tests of two-sided hypothesis

## Example: Exercise 9.4 (time permitting)

Work on whiteboard and computer.

## II. Analysis of Hands-on Monte Carlo (similar to Ch. 9, C1)

```{r}
n = 1e2
success <- c(2, 4, 5, 8)
set.seed(44)
data <- sample( 0:9, size = n, replace = T)
sum(data %in% success) / n
```

Let's return to the data collected at the start of class.

1. Compute the frequentist estimator $\hat{\pi}_f$ for each person's data.
2. Compute the Bayes' estimator $\hat{\pi}_B$ for each person's data.
3. Calculate the mean of each estimator (across people in your group).
4. Using step 3, calculate the bias of each estimator (across people in your group).
5. Calculate the variance of each estimator.
6. Calculate the mean squared error of these estimators two ways:

First way:  

$$
MSE[\hat{\pi}] = (bias(\hat{\pi}))^2 + Var[\hat{\pi}].
$$

Second way:  

Find the mean of the squared difference of your estimator and the true value.

Answer the following,

Are the estimators unbiased?
Which has smaller MSE for your $\pi$?

- Aggregate class data and discuss.

## III. Poisson inference notes and example (ch. 10)

Key concepts/formulas from the chapter:

- Gamma is the conjugate family for Poisson observations, with density proportional to $\mu^{r-1} e^{-v\mu}; \mu \geq 0, r,v > 0$. Here $r$ is the *shape* parameter and $v$ is the *rate* parameter (1/rate is the *scale* parameterization).
- If the prior is $gamma(r, v)$, then the posterior, $\mu|y$, is $gamma\left(r^\prime = \sum y + r, v^\prime = n + v \right)$.
- The posterior mean, $E[\mu|y]$, is $\frac{r^\prime}{v^\prime}$.
- The posterior mode is $\frac{r^\prime - 1}{v^\prime}$.
- The posterior variance, $Var[\mu|y]$, is $\frac{r^\prime}{{v^\prime}^2}$.
- The posterior median must be found numerically:

```{r}
r_prime <- 2
v_prime <- 1
## CAREFUL: v is the "rate" parameter in R
## not the scale (1/v).
## Let's read ?qgamma
cat("Median is ")
my_area <- 0.5
qgamma(p = my_area, shape = r_prime, rate = v_prime)
```

- Equivalent sample size is $n_{eq}=v$.
- Bias of the "best" Bayesian point estimator (the posterior mean, $\hat{\mu_B}$): $E[\hat{\mu_B}] - \mu = \frac{r - v\mu}{v + n}$.
- Variance of the "best" Bayesian point estimator: $Var[\hat{\mu_B}] = \frac{n\mu}{(v + n)^2}$.
- Positive uniform (improper) density is defined as $1, \mu \geq 0$. This can be viewed as the limit of a gamma with $r = 1, v \rightarrow 0$.
- Jeffrey's prior (improper) density is $\mu^{-1/2}, \mu > 0$. This can be viewed as the limit of a gamma with $r = 1/2, v \rightarrow 0$.

### Example: Bolstad Ch.10, 10.1

Hand example on the board and R code:

```{r}
## OUR CODE HERE
```

### You try: HW 7, Ch.10 Exercise 10.2

### Computer-based example Poisson inference
  
#### Example: Bolstad Ch.10, C2

```{r}
y = c(3, 4, 3, 0, 1)
## Jeffrey's prior
r <- 0.5
v <- 0
(r_prime <- r + sum(y))
(v_prime <- v + length(y))
## mean
r_prime / v_prime
## median
qgamma(p = 0.5, shape = r_prime, rate = v_prime)
## 95% posterior credible interval
alpha = 0.05
(lower <- qgamma(p = alpha / 2, shape = r_prime, rate = v_prime))
(upper <- qgamma(p = alpha / 2, shape = r_prime, rate = v_prime, lower.tail = FALSE))

## using Bolstad package
my_posterior <- Bolstad::poisgamp(y = y, shape = 0.5, rate = 0)
mean(my_posterior)
quantile(my_posterior, probs = 0.5)
round(quantile(my_posterior, probs = c(alpha / 2, 1 - ( alpha / 2 ) )), 2)
```

## Closing (5 min)

- What are the updating rules for conjugate-based Bayesian inference of the mean from a Poisson random variable?
- Questions about anything?
