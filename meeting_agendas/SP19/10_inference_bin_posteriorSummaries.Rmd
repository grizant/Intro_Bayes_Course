---
title: "Bayesian Inference for Binomial Proportion (Ch.8)"
author: "AG Schissler"
date: "02/27/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<bring to class: textbook, globe>

## Start-of-class work: Setting priors for the proportion of water on Earch (5 - 10 min)

Let's finish our community of priors for inferring the proportion:

```{r}
## our priors here
```

## Class demonstration: Collect data on proportion of the earth covered by water (5 - 10 min)

Let's collect data on the proportion of water covering the globe through a ball tossing experiment. 

```{r}
### our data goes here
y = 7
n = 10

Bolstad::binobp(x = 0, n = 0, a = 57.09, b = 21.66)
Bolstad::binobp(x = 7, n = 10, a = 57.09, b = 21.66)
Bolstad::binobp(x = 23, n = 40, a = 57.09, b = 21.66)

## G's GCP
pi = seq(0, 1, by = 0.001)
pi.prior = rep(0, length(pi))
priorFun = Bolstad::createPrior(x = c(0, 0.67,0.68,0.74, 0.75, 1), wt = c(0, 0, 1, 1, 0, 0))
pi.prior = priorFun(pi)
priorFit <- Bolstad::binogcp(0, 0, "user", pi = pi, pi.prior = pi.prior)
mean(priorFit)
fit <- Bolstad::binogcp(7, 10, "user", pi = pi, pi.prior = pi.prior)
fit <- Bolstad::binogcp(23, 40, "user", pi = pi, pi.prior = pi.prior)
str(fit)
mean(fit)
quantile(fit, probs = c(0.025, 0.975))
diff(quantile(fit, probs = c(0.25, 0.75)))
```

## I. Summarizing the posterior distribution (20 - 25 min)

Now let's compute the posterior distribution of $\pi$ based on our community of priors and the data. Please compute the posterior for your prior.

Some examples below:

```{r}
### our code here
```

We now have the entire posterior distribution for each prior. Often we'd like to summarize the distribution numerically to interpret and make comparisons.

### Measures of location

1. *Posterior mode*. The value that maximizes the posterior is a sensible point estimator. Careful though as some posteriors have multiple modes or a mode near the boundary of the parameter space (does not reflect the entire distribution). For a beta posterior with updated parameters $(a^{\prime}, b^{\prime})$, the mode is 

$$
\frac{a^{\prime} - 1 }{a^{\prime} + b^{\prime} - 2}.
$$

2. *Posterior median*. This is value that have 50\% of the posterior distribution above and below it. It is the solution to

$$
\int_0^{\hat{\pi}_{median}} g(\pi|y)d\pi = 0.5.
$$

3. *Posterior mean*. The expected value:

$$
m^{\prime} = \int_0^1 \pi g(\pi|y)d\pi.
$$

For a beta posterior, we have $m^{\prime} = \frac{a^{\prime}}{a^{\prime} + b^{\prime}}$.

### Measures of spread

1. *Posterior variance*:

$$
Var[\pi|y] = \int_0^1 (\pi - m^{\prime})^2 g(\pi|y)d\pi.
$$

For a beta posterior, we have 

$$
Var[\pi|y] = \frac{a^{\prime} \times b^{\prime}}{(a^{\prime} + b^{\prime})^2 \times (a^{\prime} + b^{\prime} + 1)}.
$$

2. *Posterior standard deviation*. Square root of the posterior variance. It is on the scale of units (not squared units) and can be compared to th size of the mean.

3. *Interquantile range*. $IQR = Q_3 - Q_1$ is robust to heavy tails and outliers.

### Back to example 8.1

```{r}
### IQR for Bart:
fit <- quantile(Bolstad::binobp(x = 26, n = 100))
quantile(fit, probs = c(0.25, 0.75))
unname(diff(quantile(fit, probs = c(0.25, 0.75))))
```

## II. Estimating the proportion (5 - 10 min)

Any of the measures of location above can be used as a point estimator, $\hat{\pi}$, of $\pi$. One way to justify a choice of $\hat{\pi}$ is by analyzing the  

PMSE = Posterior Mean Squared **Error** which is defined as

$$
PMSE[\hat{\pi}] = \int_0^1 (\pi - \hat{\pi})^2 g(\pi|y)d\pi.
$$

By adding and substracting the posterior mean, $m^{\prime}$ inside the squared term, we can decompose this integral into two components the **variance** and the squared **bias**.

$$
PMSE[\hat{\pi}] = Var[\pi|y] + (m^{\prime} - \hat{\pi})^2.
$$

Often estimators are evaluated by this so-called variance-bias tradeoff. Clearly, PMSE is minimized at $\hat{\pi} = m^{\prime}$. 

## III. Bayesian credible intervals (5 - 10 min)

Of course, statisticians as never satistified by a mere point estimator! We have the entire distribution, but we may desire an probability interval around our point estimator. In the Bayesian universe, we call this a credible interval (analogous to a confidence interval but MUCH easier to interpret). There are multiple ways to form these intervals

1. A highest posterior density interval. The shortest interval (or union of intervals) with the desired probability

2. A percentile-based interval (equal tails).

3. Normal approximation (appropriate when $a^{\prime}, b^{\prime} \geq 10$).

Let's look this post to [https://stats.stackexchange.com/questions/148439/what-is-a-highest-density-region-hdr](https://stats.stackexchange.com/questions/148439/what-is-a-highest-density-region-hdr)

### Back to Chris's general continuous prior

```{r}
pi = seq(0, 1, by = 0.001)
pi.prior = rep(0, length(pi))
priorFun = Bolstad::createPrior(x = c(0, 0.1, 0.3, 0.5), wt = c(0, 2, 2, 0))
pi.prior = priorFun(pi)
results <- Bolstad::binogcp(26, 100, "user", pi = pi, pi.prior = pi.prior)
alpha <- 0.05
qtls <- quantile(results, probs = c(alpha/2, 1-(alpha/2)))
cat(paste("Approximate 95% credible interval : ["
        , round(qtls[1], 4), " ", round(qtls[2], 4), "]\n", sep = ""))
```

### IV. Summarize our globe experiment under your prior (5 - 10 min)

```{r}
## Our code here
```

## Closing (5 min)

Explain in your own words what it means to summarize a posterior distribution.
