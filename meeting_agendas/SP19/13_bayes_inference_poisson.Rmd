---
title: "Bayesian inference for Binomial and Poisson (Ch.9,10)"
author: "AG Schissler"
date: "03/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<bring to class: laptop, dice, textbook>

## Start-of-class work: Finish Hands-on Monte Carlo (similar to Ch. 9, C1; 20 - 25 min)

Let's return to the data collected at the start of class.

1. Compute the frequentist estimator $\hat{\pi}_f$ for each person's data.
2. Compute the Bayes' estimator $\hat{\pi}_B$ for each person's data.
3. Calculate the mean of each estimator (across people in your group).
4. Using step 3, calculate the bias of each estimator (across people in your group).
5. Calculate the variance of each estimator.
6. Calculate the mean squared error of these estimators two ways:

First way:  

$$
MSE[\hat{\pi}] = (bias(\hat{\pi}))^2 + Var[\hat{\pi}].
$$

Second way:  

Find the mean of the squared difference of your estimator and the true value.

Answer the following,

Are the estimators unbiased?
Which has smaller MSE for your $\pi$?

Let's aggregate the class data.

## Trevor's grad talk

Please introduce your topic to the class Trevor.

## II. Hand calculation example Poisson inference (20 - 25 min)

Key concepts/formulas from the chapter:

- Gamma is the conjugate family for Poisson observations, with density proportional to $\mu^{r-1} e^{-v\mu}; \mu \geq 0, r,v > 0$. Here $r$ is the *shape* parameter and $v$ is the *rate* parameter (1/rate is the *scale* parameterization).
- If the prior is $gamma(r, v)$, then the posterior, $\mu|y$, is $gamma\left(r^\prime = \sum y + r, v^\prime = n + v \right)$.
- The posterior mean, $E[\mu|y]$, is $\frac{r^\prime}{v^\prime}$.
- The posterior mode is $\frac{r^\prime - 1}{v^\prime}$.
- The posterior variance, $Var[\mu|y]$, is $\frac{r^\prime}{{v^\prime}^2}$.
- The posterior median must be found numerically:

```{r}
r_prime <- 2
v_prime <- 1
## CAREFUL: v is the "rate" parameter in R
## not the scale (1/v).
## Let's read ?qgamma
cat("Median is ")
my_area <- 0.5
qgamma(p = my_area, shape = r_prime, rate = v_prime)
```

- Equivalent sample size is $n_{eq}=v$.
- Bias of the "best" Bayesian point estimator (the posterior mean, $\hat{\mu_B}$): $E[\hat{\mu_B}] - \mu = \frac{r - v\mu}{v + n}$.
- Variance of the "best" Bayesian point estimator: $Var[\hat{\mu_B}] = \frac{n\mu}{(v + n)^2}$.
- Positive uniform (improper) density is defined as $1, \mu \geq 0$. This can be viewed as the limit of a gamma with $r = 1, v \rightarrow 0$.
- Jeffrey's prior (improper) density is $\mu^{-1/2}, \mu > 0$. This can be viewed as the limit of a gamma with $r = 1/2, v \rightarrow 0$.

### Example: Bolstad Ch.10, 10.1

Hand example on the board and R code:

```{r}
## OUR CODE HERE
```

### You try: HW 7, Ch.10 Exercise 10.2

## III. Computer-based example Poisson inference (15 - 20 min)
  
### Example: Bolstad Ch.10, C2

```{r}
y = c(3, 4, 3, 0, 1)
## Jeffrey's prior
r <- 0.5
v <- 0
(r_prime <- r + sum(y))
(v_prime <- v + length(y))
## mean
r_prime / v_prime
## median
qgamma(p = 0.5, shape = r_prime, rate = v_prime)
## 95% posterior credible interval
alpha = 0.05
(lower <- qgamma(p = alpha / 2, shape = r_prime, rate = v_prime))
(upper <- qgamma(p = alpha / 2, shape = r_prime, rate = v_prime, lower.tail = FALSE))

## using Bolstad package
my_posterior <- Bolstad::poisgamp(y = y, shape = 0.5, rate = 0)
mean(my_posterior)
quantile(my_posterior, probs = 0.5)
round(quantile(my_posterior, probs = c(alpha / 2, 1 - ( alpha / 2 ) )), 2)
```

## Closing (5 min)

Please write down and share responses to the following prompts:

- 3 ways to evaluate an estimator
- 2 questions: Why is frequentist estimation procedure *pre-data*? And why is Bayesian *post-data*?
- Why is it not meaningful to produce a Bayesian 2-sided P-value?
