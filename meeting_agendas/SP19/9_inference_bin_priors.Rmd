---
title: "Bayesian Inference for Binomial Proportion (Ch.8)"
author: "AG Schissler"
date: "02/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<bring to class: textbook, plastic globe>


## Start-of-class work: How much of the Earth is covered by water? (5 min)

Without any research, discuss/share out how much of the Earth is covered by water. How sure are you of your guess? How could we formalize this? 

## I. Bayesian inference for a binomial proportion (5 - 10 min)

This chapter focuses on the inferring the posterior distribution of the proportion $\pi$ (probability) of success in a binomial model. By that we mean the data model (conditional probability function) is specified as $f(y|\pi)=\binom{n}{y}\pi^y(1-\pi)^{n-y}$ for $y=1,\ldots,n$. Viewed a different way, this also means the likelihood function is specified as $f(y|\pi)=\binom{n}{y}\pi^y(1-\pi)^{n-y}$ for $\pi \in [0,1]$. To infer the posterior distribution $\pi$, $g(\pi | y)$, we use Bayes' rule:

$$
g(\pi | y) = \frac{g(\pi) \times f(y|\pi)}{\int_0^1 g(\pi) \times f(y|\pi) d\pi}.
$$

Note that since the denominator is the marginal of $y$ and so is not a function of $\pi$, we see that

$$
g(\pi | y) \propto g(\pi) \times f(y|\pi). 
$$

So in practice we often compute the above product then find the normalizing constant $k$ through integration (often numeric if there is no closed form).

From the equation above, we see that since we have already specified $f(y|\pi)$ and the task now requires the specification of the prior probability function, $g(\pi)$. This is a fundamental task in Bayesian inference and we'll address this issue in detail today.

## II. Priors for $\pi$ (15 - 20 min)

First, look at the support of $\pi$. What are sensible values the random variable can take on? Also, from a pure standpoint, it's critical that $g(\pi)$ is determining before looking at the data! Note there is a class of approximate Bayesian inference called Empirical Bayes if you are interested in ways to use data to specify the prior.

### Using a Uniform prior

Using a flat prior (uniform) is common method in inferring $\pi$. Some call this an "objective" or vague or non-informative prior. But note that this actually saying quite a lot! It says all sensible values of $\pi$ are equally likely before seeing the data. Do you really believe that 0 or something close to it is equally likely as any other value for your problem?

For a uniform prior we have $g(\pi) = 1$ with $\pi \in [0,1]$. Clearly then we see that 

$$
f(\pi|y) \propto \binom{n}{y}\pi^y(1-\pi)^{n-y}, \pi \in [0,1].
$$

Stare at this for a moment. It's a beta density! So, with a uniform prior, the posterior $g(\pi|y) \sim beta(a = y + 1, b = n - y + 1)$. And your inference is essentially complete!

### More generally, using a beta prior

Recall that if $\pi$ is standard uniform then $\pi \sim beta(a = 1, b = 1)$. In order to better reflect a variety of prior beliefs, you could use any memeber of the beta family by specified $a,b$. Let's look at Figure 8.1 and discuss.

Following the arguments on page 151 when you have beta prior density (with arbitrary parameters $a,b$) and a binomial model, we have

$$
g(\pi|y) \propto \pi^{a + y - 1} (1-\pi)^{b + n - y - 1}, \pi \in [0,1].
$$

Therefore our posterior is also a beta! But now the parameters have been **updated** to $(a^{\prime}, b^{\prime})$, with

$$
a^{\prime} = a + y\\
b^{\prime} = b + n - y
$$

### Important concept: Conjugacy!

This brings up an important general concept called **conjugacy**. When you specify a prior of a certain form and once combined with the likelihood the posterior have the same form it is called the conjugate family. In this case the *beta* distribution is the conjugate family for the binomial observation distribution. 

When you use a conjugate family, you are updating the parameters after seeing the data. This is a big advantage to computation and interpretation. It is not always appropriate though and must be justified as a choice.

### Jeffrey's prior for the binomial

Jeffrey's priors are a way of constructing priors that are invariant under continuous transformations. Since we can parameterize probability densities in multiple ways, this is an appealing feature --- that we get the same inference under whatever formulation of the density. But often Jeffrey's prior is quite information. For the binomial data model, Jeffrey's prior is

$$
\pi \sim beta(0.5, 0.5).
$$

### Think-pair-share (5 - 10 min)

My prior belief is that $\pi$ has mean of 0.25 with standard deviation of 0.1. What $beta$ density reflects that belief? What is my equivalent sample size?

## III. Choosing your prior (10 - 15 min)

When selecting a prior, always graph, compute moments (mean and standard deviation), and the *equivalent sample size* before combining with the data to make sure that you have properly specified your beliefs.

### Graph your prior

An easy way to graph your beta prior using the `Bolstad` package. For example let's graph and discuss Jeffrey's prior:

```{r}
Bolstad::binobp(x = 0, n = 0, a = 0.5, b = 0.5)
```

### Compute moments

$$
E(\pi) = \frac{a}{a + b}\\
Var(\pi) = \frac{ab}{(a+b)^2 \times (a + b + 1)} 
$$

### Find the equivalent sample size

Following the arguments on page 155, the equivalent sample size of beta prior is $n_{eq} = a + b + 1$.

### Choosing a conjugate prior by matching moments from expert knowledege

Formal prior elicitation is hugely important when working with domain experts. Often the researcher has some concept of the mean and variance, but not the entire beta shape. When you specify the two moments you can then solve for $a,b$ using standard systems of equations techniques (two unknowns, two equations).

### Constructing a General Continuous Prior

There are cases where the prior belief is best specified not be a $beta$ but by some arbitrary shape. One can construct a general continuous prior by linear interpolation.

For example, let's construct "Chris's prior" in Example 8.1 using the `Bolstad` package:

```{r}
## for help
## ?Bolstad::binogcp
pi = seq(0, 1, by = 0.001)
pi.prior = rep(0, length(pi))
priorFun = Bolstad::createPrior(x = c(0, 0.1, 0.3, 0.5), wt = c(0, 2, 2, 0))
pi.prior = priorFun(pi)
Bolstad::binogcp(0, 0, "user", pi = pi, pi.prior = pi.prior)
```

### Effect of prior

An important notion is that of a "community of priors" (see *Bayesian Approaches to Randomized Trials* by Speigelhalter et al. 1994 for an excellent discussion). Often it is best to specify a broad range of priors to explore different scenarios. But in light of a lot of data, the posterior is often nearly the same! 

In Example 8.1,

```{r, fig.height = 10}
par(mfrow = c(3, 1))
## Anna's prior
Bolstad::binobp(x = 26, n = 100, a = 4.8, b = 19.2)
## Bart's prior
Bolstad::binobp(x = 26, n = 100, a = 1, b = 1)
## Chris's prior
Bolstad::binogcp(x = 26, n = 100, "user", pi = pi, pi.prior = pi.prior)
par(mfrow = c(1, 1))
```

## IV. Class demonstration: Specify your prior on proportion of the earth covered by water (10 min)

Let's formalize our initial guesses into a community of priors!

```{r}
## our priors here
```

## Closing Ticket out of the door (5 min)

Please write responses to these questions and hand in as you leave:
1. What is the most important part of today's lesson?
2. What was the muddiest part?
