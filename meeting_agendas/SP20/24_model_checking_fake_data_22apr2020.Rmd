---
title: Simulation for checking statistical procedures and model fits
author: Presented by AG Schissler
date:  04/22/2020
output:
  html_document
---

## I. Opening 

```{r, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE, cache.comments=TRUE)
set.seed(04242020)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(arm)
library(R2OpenBUGS)
library(foreign)
library(lindia)
library(knitr)
```

### Start-of-class work

## I. Opening (5-10 minutes)

Today I'll introduce the concept of using simulation to check code and statistical procedures --- and to assess how well a model fits the data. `R` has some great features to conduct these studies. Why simulate? Welcome to the 21st century! Two reasons:  
  1) Often, simulations can be easier than hand calculations.  
  2) Often, simulations can be made more realistic than hand calculations.  

### HW 11 discussion and hints

#### question 3 `heights` data

In question 3, I ask you to select a model using any combination of three variables: `sex` (or recoded as `male`), `height`, and `yearbrn`. Let's explore the year born variable together and I can make some recommendations.

```{r yearbn}
## import the Stata file
heights <- foreign::read.dta("/Users/alfred/OneDrive - University of Nevada, Reno/Teaching/STAT_446_646/ARM_Data/earnings/heights.dta")

## sex to recode to 1 and 0
heights$male <- 2 - heights$sex
## (for simplicity) remove cases with missing data
ok <- !is.na (heights$earn+heights$height+heights$sex) & heights$earn>0
heights.clean <- data.frame(earn = heights$earn, height = heights$height, male = heights$male, yearbn = heights$yearbn)[ok, ]

## Year born is tricky. Let's explore it.
qplot( x = yearbn, y = earn, data = heights.clean)
head(heights.clean[ order(heights.clean$yearbn, decreasing = T) , ])

## I think the 99 actually means 1899. Let's convert year born to age at 1990 (so that everyone is at least 18)
heights.clean$yearbn[heights.clean$yearbn == 99]  <- 1899
heights.clean$yearbn[heights.clean$yearbn != 1899]  <- heights.clean$yearbn[heights.clean$yearbn != 1899] + 1900
heights.clean$age <- 1990 - heights.clean$yearbn
summary(heights.clean$age)

```

From here, you may choose to work with the continuous variable age or discretize into categories (like the congressional example in our last lesson). My advice is to keep the number of categories small (3 or 4) if you think it is better to bin into categories to capture generational trends. For example,

```{r binningAges}
## You may like the R base function cut()
heights.clean$ageGroup <- cut( heights.clean$age, breaks = c(-Inf, 29, 44, 64, Inf), labels =  c( '18to29', '30to44', '45to64', '65up' ) )
kable( t(table( heights.clean$ageGroup) ))
```

### Fake-data simulation

- Checking code and statistical procedure entails using simulation strategies to produce synthetic data (aka *fake data* or *frauda*) then fit a model to these data.  
- The results should roughly align with the specified (true) values. This gives you confidence that the computer program is working.  
- The results should align with expected (theoretical) results. This can assess whether the model is performing as advertised. For example, what are the empricial coverage rates? 

### Predictive simulations

- I'll describe the use of *predictive simulations* to check how well a model fits data.  
- This invovles fitting a statistical model to real data (as opposed to fitting a model to fake data, as above).  
- Then repeated simulations are conducted using the model and covariates to generate predicted responses.
- These replicated data sets are then compared visually and more formally (numerically) to the original data.

### Notes

Most of this lecture is derived from Chapters 7 and 8 from the outstanding textbook *Data Analysis Using Regression and Multilevel/Hierachical Models* by Gelman and Hill 2007. We'll proceed through a series of examples of increasing complex models with code provided in `R`. The useful `R` package `arm` will do must of the heavy lifting (coding) for us.

## Ch. 7 Simulation of probability models and statistical inferences

Let's complicate the model from the beginning of class. Let's further assume there is a 1/125 chance that a birth results in fraternal twins and of which there is an approximate 49.5% chance of the twin being a girl, and a 1/300 chance of identical twins, which have an approximate 49.5% chance of being girls. We simulate 400 births as follows:

```{r ch7twins}

## Accounting for twins
birth.type <- sample (c("fraternal twin", "identical twin", "single birth"),
                      size=400, replace=TRUE, prob=c(1/25, 1/300, 1 - 1/25 - 1/300))
girls <- rep (NA, 400)
for (i in 1:400){
    if (birth.type[i]=="single birth"){
        girls[i] <- rbinom (1, 1, .488)}
    else if (birth.type[i]=="identical twin"){
            girls[i] <- 2*rbinom (1, 1, .495)}
    else if (birth.type[i]=="fraternal twin"){
                girls[i] <- rbinom (1, 2, .495)}
}
n.girls <- sum (girls)
print(n.girls)
```

```{r ch7twinsb}
## putting in a loop

n.sims <- 1000
n.girls <- rep (NA, n.sims)
for (s in 1:n.sims){
    birth.type <- sample (c("fraternal twin", "identical twin", "single birth"),
                          size=400, replace=TRUE, prob=c(1/25, 1/300, 1 - 1/25 - 1/300))
    girls <- rep (NA, 400)
    for (i in 1:400){
        if (birth.type[i]=="single birth"){
            girls[i] <- rbinom (1, 1, .488)}
        else if (birth.type[i]=="identical twin"){
                girls[i] <- 2*rbinom (1, 1, .495)}
        else if (birth.type[i]=="fraternal twin"){
                    girls[i] <- rbinom (1, 2, .495)}
    }
    n.girls[s] <- sum (girls)
}

par(mfrow = c(1,1))
hist(n.girls)
mean(n.girls); sd(n.girls)

## or more compactly
## girls <- ifelse (birth.type=="single birth", rbinom (400, 1, .488),
##                  ifelse (birth.type=="identical twin", 2*rbinom (400, 1, .495),
##                         rbinom (400, 2, .495)))
```

### 7.1 Simulation of probability models

#### An example of continuous predictive simulations

Now let's practice simulating continuous random variables. Suppose 52% of adults in the US are women and 48% are men. The heights of the men are approximately normally distributed with mean 69.1 incheds and standard deviation of 2.9 inches; women with a mean of 63.7 in. and sd 2.7 in. Let's simulate selecting 10 adults at random. What can we say about their average height?

```{r sect71}
woman <- rbinom (10, 1, .52)
height <- ifelse (woman==0, rnorm (10, 69.1, 2.9), rnorm (10, 64.5, 2.7))
avg.height <- mean (height)
print(avg.height)
```

To simulate the distribution of average heights:

```{r sect71b}
## simulation & Figure 7.1
n.sims <- 1000
avg.height <- rep (NA, n.sims)
for (s in 1:n.sims){
  sex <- rbinom (10, 1, .52)
  height <- ifelse (sex==0, rnorm (10, 69.1, 2.9), rnorm (10, 64.5, 2.7))
  avg.height[s] <- mean (height)
}
hist (avg.height, main="Average height of 10 adults") 
```

What about the maximum height?

```{r sect71c}
## simulation for the maximum height

n.sims <- 1000
max.height <- rep (NA, n.sims)
for (s in 1:n.sims){
  sex <- rbinom (10, 1, .52)
  height <- ifelse (sex==0, rnorm (10, 69.1, 2.9), rnorm (10, 64.5, 2.7))
  max.height[s] <- max (height)
}
hist (max.height, main="Maximum height of 10 adults")
```

To simulate using custom-made functions

```{r sect71d}
Height.sim <- function (n.adults){
  sex <- rbinom (n.adults, 1, .52)
  height <- ifelse (sex==0, rnorm (10, 69.1, 2.9), rnorm (10, 64.5, 2.7))
  return (mean(height))
}

avg.height <- replicate (1000, Height.sim (n.adults=10))
hist (avg.height, main="Average height of 10 adults")
```

### 7.2 Summarizing linear regression using simulation: an informal Bayesian approach

In a regression setting, we can use simulation to capture both predictive uncertainty and inferential (estimation) uncertainty.We first discuss the simplest case of simulating prediction errors, then consider inferential uncertainty and the **combination of both sources**.

#### Simulation to represent predictive uncertainty

We return to the `heights.dta` data and illustrate predictive uncertainty by considering the earnings of 68-inch-tall US male.

```{r sect72, include = TRUE}
rm(male, height)
log.earn = log(heights.clean$earn)
suppressMessages(attach.all (heights.clean))

## Model of log earnings with interactions

earn.logmodel.3 <- lm (log.earn ~ height + male + height:male)
display (earn.logmodel.3)
```

```{r sect72b}
x.new <- data.frame (height=68, male=1)
pred.interval <- predict (earn.logmodel.3, x.new, interval="prediction", 
                          level=.95)

pred.interval <- predict (earn.logmodel.3, interval="prediction", level=.95)
print (exp (pred.interval))
```

```{r sect72c}

## Constructing the predictive interval using simulation
8.39 + (0.02*68) - (0.08*1) + 0.01*68*1
pred <- exp (rnorm (1000, 10.35, .88))
pred.original.scale <- rnorm (1000, 9.95, .88)

## Histograms (Figure 7.2)

par (mfrow=c(1,2))
hist (pred.original.scale, xlab="log(earnings)", main="")
hist (pred, xlab="earnings", main="")
```

#### Why do we need simulation for predictive inferences?

```{r sect72d}

pred.man <- exp (rnorm (1000, 8.4 + 0.17*68 - 0.079*1 + .007*68*1, .88))
pred.woman <- exp (rnorm (1000, 8.4 + 0.17*68 - 0.079*0 + .007*68*0, .88))
pred.diff <- pred.man - pred.woman
pred.ratio <- pred.man/pred.woman
```

#### Simulation to represent uncertainty in regression coefficients

```{r 72e}
n.sims <- 1000
fit.1<- lm (log.earn ~ height + male + height:male)
sim.1 <- sim (fit.1, n.sims)

height.coef <- sim.1@coef[,2]
mean (height.coef)
sd (height.coef)
quantile (height.coef, c(.025, .975))

height.for.men.coef <- sim.1@coef[,2] + sim.1@coef[,4]
quantile (height.for.men.coef, c(.025, .975))

## Inside the sim function
## I admit the details to focus on the concept
## Note the use of multivariate normal
##for (s in 1: n.sims){
##  sigma[s] <- sigma.hat*sqrt((n-k)/rchisq (1, n-k))
##  beta[s] <- mvrnorm (1, beta.hat, V.beta*sigma[s]^2)
##}
##return (list (coef=beta, sigma=sigma))
```

### 7.3 Simulation for nonlinear predictions: congressional elections

#### Background and data issues

```{r sect73}
## Data are at http://www.stat.columbia.edu/~gelman/arm/examples/cong3

congress <- vector ("list", 49)
for (i in 1:49){
    year <- 1896 + 2*(i-1)
    file <- paste ("data/cong3/", year, ".asc", sep="")
    data.year <- matrix (scan (file, quiet = T), byrow=TRUE, ncol=5)
    data.year <- cbind (rep(year, nrow(data.year)), data.year)
    congress[[i]] <- data.year
}

## Note: download all ".asc" files into your R working directory in a file
## named cong3 for the above command to work

i86 <- (1986-1896)/2 + 1
cong86 <- congress[[i86]]
cong88 <- congress[[i86+1]]
cong90 <- congress[[i86+2]]

v86 <- cong86[,5]/(cong86[,5]+cong86[,6])
bad86 <- cong86[,5]==-9 | cong86[,6]==-9
v86[bad86] <- NA
contested86 <- v86>.1 & v86<.9
inc86 <- cong86[,4]

v88 <- cong88[,5]/(cong88[,5]+cong88[,6])
bad88 <- cong88[,5]==-9 | cong88[,6]==-9
v88[bad88] <- NA
contested88 <- v88>.1 & v88<.9
inc88 <- cong88[,4]

v90 <- cong90[,5]/(cong90[,5]+cong90[,6])
bad90 <- cong90[,5]==-9 | cong90[,6]==-9
v90[bad90] <- NA
contested90 <- v90>.1 & v90<.9
inc90 <- cong90[,4]

jitt <- function (x,delta) {x + runif(length(x), -delta, delta)}

## Plot Figure 7.3

v88.hist <- ifelse (v88<.1, .0001, ifelse (v88>.9, .9999, v88))
hist (v88.hist, breaks=seq(0,1,.05),
      xlab="Democratic share of the two-party vote", ylab="", yaxt="n",
      cex.axis=1.1, cex.lab=1.1, cex.main=1.2, 
      main="Congressional elections in 1988")
```

#### Fitting the model

```{r 73b}
v86.adjusted <- ifelse (v86<.1, .25, ifelse (v86>.9, .75, v86))
vote.86 <- v86.adjusted[contested88]
incumbency.88 <- inc88[contested88]
vote.88 <- v88[contested88]

fit.88 <- lm (vote.88 ~ vote.86 + incumbency.88)
display (fit.88)

## Figure 7.4

## 7.4 (a)

par (mfrow=c(1,1))
par (pty="s", mar=c(5,5,4,1)+.1)
plot (0, 0, xlim=c(0,1), ylim=c(0,1), type="n",
      xlab="Democratic vote share in 1986", ylab="Democratic vote share in 1988",
      cex.lab=1)
abline (0,1, lwd=.5)
j.v86 <- ifelse (contested86, v86, jitt (v86, .02))
j.v88 <- ifelse (contested88, v88, jitt (v88, .02))
points (j.v86[inc88==0], j.v88[inc88==0], pch=1)
points (j.v86[inc88==1], j.v88[inc88==1], pch=16)
points (j.v86[inc88==-1], j.v88[inc88==-1], pch=4)
mtext ("Raw data (jittered at 0 and 1)", line=1, cex=1.2)

## 7.4 (b)

par (pty="s", mar=c(5,5,4,1)+.1)
plot (0, 0, xlim=c(0,1), ylim=c(0,1), type="n",
      xlab="Democratic vote share in 1986", ylab="Democratic vote share in 1988",
      cex.lab=1)
abline (0,1, lwd=.5)
v86.adjusted <- ifelse (v86<.1, .25, ifelse (v86>.9, .75, v86))
vote.86 <- v86.adjusted[contested88]
vote.88 <- v88[contested88]
incumbency.88 <- inc88[contested88]
points (vote.86[incumbency.88==0], vote.88[incumbency.88==0], pch=1)
points (vote.86[incumbency.88==1], vote.88[incumbency.88==1], pch=16)
points (vote.86[incumbency.88==-1], vote.88[incumbency.88==-1], pch=4)
mtext ("Adjusted data (imputing 0's and 1's to .75)", line=1, cex=1.2)
```

#### Simulation for inferences and predictions of new data points

```{r 73c}
incumbency.90 <- inc90
vote.88 <- v88
n.tilde <- length (vote.88)
X.tilde <- cbind (rep (1, n.tilde), vote.88, incumbency.90)

n.sims <- 1000
sim.88 <- sim (fit.88, n.sims)
y.tilde <- array (NA, c(n.sims, n.tilde))
for (s in 1:n.sims){
    pred <- X.tilde %*% sim.88@coef[s,]
    ok <- !is.na(pred)
    y.tilde[s,ok] <- rnorm (sum(ok), pred[ok], sim.88@sigma[s])
}

## Predictive simulation for a nonlinear function of new data

y.tilde.new <- ifelse (y.tilde=="NaN", 0, y.tilde)
dems.tilde <- rowSums (y.tilde.new > .5)

## or
dems.tilde <- rep (NA, n.sims)
for (s in 1:n.sims){
    dems.tilde[s] <- sum (y.tilde.new[s,] > .5)
}
```

#### Implementation using functions

```{r 74d}
Pred.88 <- function (X.pred, lm.fit){
    sim.88 <- sim (lm.fit, 1)
    pred <- X.tilde %*% t(sim.88@coef)
    ok <- !is.na(pred)
    n.pred <- length (pred)
    y.pred <- rep (NA, n.pred)
    y.pred[ok] <- rnorm (sum(ok), pred[ok], sim.88@sigma)
    return (y.pred)
}

y.tilde <- replicate (1000, Pred.88 (X.tilde, fit.88))
dems.tilde <- replicate (1000, Pred.88 (X.tilde, fit.88) > .5)
```

## Ch. 8 fake data from models

The key concept in fake-data simulation for model checking is to **simulate from a statistical model known parameter values/functional form and check the properties of the procedure against theory**.

### Example 1: Simple linear regression (SLS) with Gaussian errors

```{r, 81, warning=F}
## Fake-data simulation
library("arm")
a <- 1.4
b <- 2.3
sigma <- 0.9
x <- 1:5
n <- length(x)

# Simulate data, fit the model, and check the coverage of the conf intervals

y <- a + b*x + rnorm (n, 0, sigma)
lm.1 <- lm (y ~ x)
display (lm.1)

b.hat <- coef (lm.1)[2]       # "b" is the 2nd coef in the model
b.se <- se.coef (lm.1)[2]     # "b" is the 2nd coef in the model

cover.68 <- abs (b - b.hat) < b.se     # this will be TRUE or FALSE
cover.95 <- abs (b - b.hat) < 2*b.se   # this will be TRUE or FALSE
cat (paste ("68% coverage: ", cover.68, "\n"))
cat (paste ("95% coverage: ", cover.95, "\n"))

# Put it in a loop

n.fake <- 1000
cover.68 <- rep (NA, n.fake)
cover.95 <- rep (NA, n.fake)
for (s in 1:n.fake){
  y <- a + b*x + rnorm (n, 0, sigma)
  lm.1 <- lm (y ~ x)
  b.hat <- coef (lm.1)[2]      
  b.se <- se.coef (lm.1)[2]   
  cover.68[s] <- abs (b - b.hat) < b.se  
  cover.95[s] <- abs (b - b.hat) < 2*b.se 
}
cat (paste ("68% coverage: ", mean(cover.68), "\n"))
cat (paste ("95% coverage: ", mean(cover.95), "\n"))

# Do it again, this time using t intervals

n.fake <- 1000
cover.68 <- rep (NA, n.fake)
cover.95 <- rep (NA, n.fake)
t.68 <-  qt (.84, n-2)
t.95 <-  qt (.975, n-2)
for (s in 1:n.fake){
  y <- a + b*x + rnorm (n, 0, sigma)
  lm.1 <- lm (y ~ x)
  b.hat <- coef (lm.1)[2]      
  b.se <- se.coef (lm.1)[2]   
  cover.68[s] <- abs (b - b.hat) < t.68*b.se  
  cover.95[s] <- abs (b - b.hat) < t.95*b.se 
}
cat (paste ("68% coverage: ", mean(cover.68), "\n"))
cat (paste ("95% coverage: ", mean(cover.95), "\n"))  
```

### Example 2: Using simulation to understand (SLS) residual plots

```{r, 82}
## Read in the data
# Data are at http://www.stat.columbia.edu/~gelman/arm/examples/simulation

## library ("arm")
tmp_dir <- "~/OneDrive - University of Nevada, Reno/Teaching/Gelman_and_Hill_2007_code_data"

grades <- read.table(file.path(tmp_dir, "gradesW4315.dat"), header=TRUE)
midterm <- grades[,"Midterm"]
final <- grades[,"Final"]

## Estimate the model

lm.1 <- lm (final ~ midterm)
display (lm.1)

## Construct fitted values

n <- length(final)
X <- cbind (rep(1,n), midterm)
predicted <- X %*% coef (lm.1)
resid <- lm.1$residuals

## Simulate fake data & compute fitted values

a <- 65
b <- 0.7
sigma <- 15
y.fake <- a + b*midterm + rnorm (n, 0, sigma)

lm.fake <- lm (y.fake ~ midterm)
predicted.fake <- X %*% coef (lm.fake)
resid.fake <- y.fake - predicted.fake

par (mfrow=c(2,2))

## Plots figure 8.1

 # plot on the left

plot (predicted, resid, xlab="predicted value", ylab="residual",
  main="Residuals vs. predicted values", pch=20)
abline (0,0, col="gray", lwd=.5)

 # plot on the right

plot (final, resid, xlab="observed value", ylab="residual",
  main="Residuals vs. observed values", pch=20)
abline (0,0, col="gray", lwd=.5)

## Plots figure 8.2

 # plot on the left

plot (predicted.fake, resid.fake, xlab="predicted value", ylab="residual",
  main="Fake data: resids vs. predicted", pch=20)
abline (0,0, col="gray", lwd=.5)

 # plot on the right

plot (y.fake, resid.fake, xlab="observed value", ylab="residual",
  main="Fake data: resids vs. observed", pch=20)
abline (0,0, col="gray", lwd=.5)

```

## III. Predictive simulation (20 - 30 min)

The key concept in predictive simulation for model checking is to **simulate from the fitted model and compare to the actual data**.

### Example 3: Comparing data to replications from a fitted 

Data from Newcomb's famous 1882 experiment to measure the speed of light.

```{r, light, eval=T, message=F, warning=F, fig.caption="Histogram of Simon Newcomb's measurements for estimating the speed of light, from Stigler (1977). The data represent the amount of time required for light to travel a distance of 7442 meters and are recorded as deviations from 24,800 nanoseconds."}
## Read in the data
# Data are at http://www.stat.columbia.edu/~gelman/arm/examples/lightspeed
## library ("arm")

y <- scan(file.path(tmp_dir, "lightspeed.dat"), skip=4)

## Display original data
hist(y, xlab="", ylab="", cex.main="1", yaxt="n", xlim=range(y))
```

```{r, light2, eval=T, message=F, warning=F, fig.caption=""}
## Model fit 

light <- lm (y ~ 1)
display (light)

## Create the replicated data 

n.sims <- 1000
sim.light <- sim (light, n.sims)

## Create fake data 

n <- length (y)
y.rep <- array (NA, c(n.sims, n))
for (s in 1:n.sims){
  y.rep[s,] <- rnorm (n, sim.light@coef[s], sim.light@sigma[s])
}

## Histogram of replicated data (Figure 8.4)

## par (mfrow=c(5,4), mar=c(3,1,2,1))
## for (s in 1:20){
##   hist (y.rep[s,], xlab="", ylab="", cex.main="1", yaxt="n", xlim=range(y))
##}
```

```{r, light3, eval=T, message=F, warning=F, fig.caption=""}
## Write a function to make histograms with specified bin widths and ranges

Hist.preset <- function (a, width, ...){
  a.hi <- max (a, na.rm=TRUE)
  a.lo <- min (a, na.rm=TRUE)
  if (is.null(width)) width <- min (sqrt(a.hi-a.lo), 1e-5)
  bin.hi <- width*ceiling(a.hi/width)
  bin.lo <- width*floor(a.lo/width)
  hist (a, breaks=seq(bin.lo,bin.hi,width), ...)
}

## Run the function

par (mfrow=c(5,4), mar=c(3,1,2,1))
for (s in 1:20){
  Hist.preset (y.rep[s,], width=5, xlab="", ylab="", cex.main="1", yaxt="n",
               xlim=range(y), main=paste("Replication #",s,sep=""))
}

```

```{r, light4, eval=T, message=F, warning=F, fig.caption=""}
## Numerical test

Test <- function (y){
  min (y)
}
test.rep <- rep (NA, n.sims)
for (s in 1:n.sims){
  test.rep[s] <- Test (y.rep[s,])
}

## Histogram Figure 8.5

par (mfrow=c(1,1))
hist (test.rep, xlim=range (Test(y), test.rep), yaxt="n", ylab="",
 xlab="", main="Observed Min(y) and distribution of Min(y.rep)")
lines (rep (Test(y), 2), c(0,10*n))
```

### Example 4: Check the fit of a time-series model

```{r unemp1, eval=T}
## Read in the data
# Data are at http://www.stat.columbia.edu/~gelman/arm/examples/unemployment
## library ("arm")

unemployment <- read.table (file.path(tmp_dir, "unemployment.dat"), header=TRUE)
year <- unemployment$year
y <- unemployment$unemployed.pct

## Plot of the unemployment rate

par (mar=c(4,4,2,2))
plot (year, y, type="l", ylab="unemployment", xlab="year", yaxs="i",
  ylim=c(0, max(y)*1.05), yaxt="n", mgp=c(2,.5,0), cex.axis=1.2, cex.lab=1.2)
axis (2, c(0,5,10), paste (c(0,5,10), "%", sep=""), mgp=c(2,.5,0), cex.axis=1.2)

## Fitting a 1st-order autogregression

n <- length (y)
y.lag <- c (NA, y[1:(n-1)])
lm.lag <- lm (y ~ y.lag)
display (lm.lag)

myData <- data.frame( y = y, yLag = y.lag )
cor(myData, use = 'complete.')

library(rstanarm)
stan_glm( 
```

```{r, unemp2, eval=T, message=F, warning=F, fig.caption=""}
## Simulating replicated datasets 

b.hat <- coef (lm.lag)        # vector of 2 regression coefs
s.hat <- sigma.hat (lm.lag)   # residual sd

n.sims <- 1000
y.rep <- array (NA, c(n.sims, n))
for (s in 1:n.sims){
  y.rep[s,1] <- y[1]
  for (t in 2:n){
    prediction <- c (1, y.rep[s,t-1]) %*% b.hat
    y.rep[s,t] <- rnorm (1, prediction, s.hat)
  }
}

## Including uncertainty in the estimated parameters

lm.lag.sim <- sim (lm.lag, n.sims)       # simulations of beta and sigma
for (s in 1:n.sims){
  y.rep[s,1] <- y[1]
  for (t in 2:n){
    prediction <-  c (1, y.rep[s,t-1]) %*% lm.lag.sim@coef[s,]
    y.rep[s,t] <- rnorm (1, prediction, lm.lag.sim@sigma[s])
  }
}
```

```{r, unemp3, eval=T, message=F, warning=F, fig.caption="", fig.height=10}
## Plot of simulated unemployment rate series

par (mfrow=c(5,3), mar=c(4,4,2,2))
for (s in 1:15){
  plot (year, y.rep[s,], type="l", ylab="unemployment", xlab="year", yaxs="i",
  ylim=c(0, max(y)*1.05), yaxt="n", mgp=c(2,.5,0),
        main=paste("simulation #", s, sep=""), cex.main=0.95)
  axis (2, c(0,5,10), paste (c(0,5,10), "%", sep=""), mgp=c(2,.5,0))
}
```

```{r, unemp4, eval=T, message=F, warning=F}
## Numerical model check

Test <- function (y){
  n <- length (y)
  y.lag <- c (NA, y[1:(n-1)])
  y.lag2 <- c (NA, NA, y[1:(n-2)])
  sum (sign(y-y.lag) != sign(y.lag-y.lag2), na.rm=TRUE)
}

n.sims <- 1000
print (Test (y))
test.rep <- rep (NA, n.sims)
for (s in 1:n.sims){
  test.rep[s] <- Test (y.rep[s,])
}

print (mean (test.rep > Test(y)))
print (quantile (test.rep, c(.05,.5,.95)))
```

## IV. Closing activity (5 - 10 min)

### Think-pair-share

1. Think: Where/how you could apply today's concepts and skills to one of your data analytic problems.
2. Pair: Refine your thoughts through discussion with a partner or small-group near you.
3. Share: Share your combined thoughts with the whole group.

### Thanks for your attention and participation!

Let me know if you have any questions.

