---
title: "Multilevel Regression: Examples and Basics"
author: "Presented by AG Schissler"
date: "04/27/2020"
output: 
  html_vignette:
    toc: yes
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE, cache.comments=TRUE)
suppressMessages(library(ggplot2))
suppressMessages(library(arm))
suppressMessages(library(rstanarm))
suppressMessages(library(lme4)) ## for lmer
suppressMessages(library(foreign))
```

## Today's lesson is derived from Gelman and Hill Ch. 11, 12

Most of these notes are direct quotes, paraphrases, and adaptations from 

[Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill 2007)](http://www.stat.columbia.edu/~gelman/arm/)

Your exercises will also be derived from the same textbook. You can find data sets and examples here:

http://www.stat.columbia.edu/~gelman/arm/software/

##  Ch. 11 Multilevel structures

### 11.1 Varying-intercept and varying-slope models

With groups data, a regression that includes indicators for groups in called a *varying-intercept model* because it can be interpreted as a model with a different intercept for each group. 

Varying-intercept model:  
$$
y_i = \alpha_{j[i]} + \beta x_i + \epsilon_i.
$$

Varying-slope model:  
$$
y_i = \alpha + \beta_{j[i]} x_i + \epsilon_i.
$$

Varying-intercept and varying-slope model:  
$$
y_i = \alpha_{j[i]} + \beta_{j[i]} x_i + \epsilon_i.
$$

### 11.4 Indicator variables and fixed / random effects

- One can specify and fit a multilevel model in a classical way: Include $J-1$ indicator variables it a standard linear regression. But this can cause problems in estimating these effects. It is better to specify and estimate these quantities using multilevel model software.

- Note that the definitions/notions of fixed/random effects from classical "mixed effects" literature and applied regression textbook descriptions are confusing and contradictory. Using phrases like "multilevel model with varying intercepts and varying slopes" is more clear and modern.

## Ch. 12 Multilevel linear models: the basics

It is useful to think about multilevel modeling as a compromise between complete pooling and no pooling. We call this *partial pooling*. No pooling means analyze the individual measurements separately (ignoring the group structure). Complete pooling means ignoring individual variation. For example, average together the individual measurements and fit a model using these summaries as the "observations". 

### 12.1 Notation

* Units. $i = 1, \ldots, n$. By *units* we mean the smallest items of measurement.
* Outcome measurements $y = (y_i, \ldots, y_n)$. These are the unit-level data being modeled.
* Regression predictors represented by an $n \times k$ matrix $X$. So that the predicted values are $\hat{y} = X \beta$.
* For each individual unit $i$, we denote its row vector of predictors as $X_i$.

### 12.2 Partial pooling with no predictors

```{r sect12point2, eval = FALSE}

## Using the better maintained rstanarm repo
# loads packages, creates ROOT, SEED, and DATA_ENV
demo("SETUP", package = "rstanarm", verbose = FALSE, echo = FALSE, ask = FALSE)

### Radon data
source(paste0(ROOT, "ARM/Ch.12/radon.data.R"), local = DATA_ENV, verbose = FALSE) 
radon <- with(DATA_ENV, data.frame(y, x, u, radon, county))

radon$county <- factor(radon$county)
## varying-intercept model, no predictors
M0_stan <- stan_lmer(y ~ 1 + (1 | county), data = radon, seed = SEED, refresh = REFRESH)
print(M0_stan)
```

### 12.3 Partial pooling with predictors

```{r sect 12point3}

## Complete pooling regression
lm.pooled <- lm( y ~ x, data = radon)
radon$pred_pooled <- predict( lm.pooled )
## display (lm.pooled)

## No pooling regression
lm.unpooled <- lm (y ~ x + county - 1, data = radon)
radon$pred_unpooled <- predict( lm.unpooled )
## display (lm.unpooled)

display8 <- c (36, 1, 35, 21, 14, 71, 61, 70)  # counties to be displayed

radon8 <- radon[ radon$county %in% display8, ]

p <- ggplot( data = radon8, aes( x = x, y = y) ) 
p <- p + geom_point() + facet_wrap( ~ county, nrow = 2 )
p <- p + geom_line( aes(y = pred_pooled), linetype = 2 ) 
p <- p + geom_line( aes(y = pred_unpooled), linetype = 1 )
p
```

### 12.4 Quickling fitting multilevel predictors in R

```{r sect12point4}

## Read the data
# Data are at http://www.stat.columbia.edu/~gelman/arm/examples/radon

# The R codes & data files should be saved in the same directory for
# the source command to work

## source("12.3_Partial pooling with predictors.R") # where variables were defined
# close the Bugs window to proceed

## Varying-intercept model w/ no predictors
M0 <- lmer( y ~ 1 + (1 | county), data = radon)
## display (M0)

## Including x as a predictor
M1 <- lmer (y ~ x + (1 | county), data = radon)
radon$pred_paritalPooled <- predict( M1 )

## head(display (M1))

  # estimated regression coefficicents
## coef (M1)

  # fixed and random effects
## fixef (M1)
## ranef (M1)

  # uncertainties in the estimated coefficients
## se.fixef (M1)
## se.ranef (M1)

  # 95% CI for the slope
fixef(M1)["x"] + c(-2,2)*se.fixef(M1)["x"]
#or
fixef(M1)[2] + c(-2,2)*se.fixef(M1)[2]

  # 95% CI for the intercept in county 26
coef(M1)$county[26,1] + c(-2,2)*se.ranef(M1)$county[26]

  # 95% CI for the error in the intercept in county 26
as.matrix(ranef(M1)$county)[26] + c(-2,2)*se.ranef(M1)$county[26]

## to plot Figure 12.2
plot( M0_stan )

## to plot Figure 12.4
## partial pooling as well
radon8 <- radon[ radon$county %in% display8, ]
p <- ggplot( data = radon8, aes( x = x, y = y) ) 
p <- p + geom_point() + facet_wrap( ~ county, nrow = 2 )
p <- p + geom_line( aes(y = pred_pooled), linetype = 2 ) 
p <- p + geom_line( aes(y = pred_unpooled), linetype = 1 )
p + geom_line( aes(y = pred_paritalPooled), linetype = 1, color = 'red' )

```



### 12.6 Group-level predictors

```{r sect12point6}

## Read the data
# Data are at http://www.stat.columbia.edu/~gelman/arm/examples/radon

# The R codes & data files should be saved in the same directory for
# the source command to work

## source("12.3_Partial pooling with predictors.R") # where variables were defined
# close the Bugs window to proceed

srrs2 <- read.table ("srrs2.dat", header=T, sep=",")
mn <- srrs2$state=="MN"
radon <- srrs2$activity[mn]
log.radon <- log (ifelse (radon==0, .1, radon))
floor <- srrs2$floor[mn]       # 0 for basement, 1 for first floor
n <- length(radon)
y <- log.radon
x <- floor

# get county index variable
county.name <- as.vector(srrs2$county[mn])
uniq <- unique(county.name)
J <- length(uniq)
county <- rep (NA, J)
for (i in 1:J){
  county[county.name==uniq[i]] <- i
}

## Get the county-level predictor
srrs2.fips <- srrs2$stfips*1000 + srrs2$cntyfips
cty <- read.table ("cty.dat", header=T, sep=",")
usa.fips <- 1000*cty[,"stfips"] + cty[,"ctfips"]
usa.rows <- match (unique(srrs2.fips[mn]), usa.fips)
uranium <- cty[usa.rows,"Uppm"]
u <- log (uranium)

## Varying-intercept model w/ group-level predictors
u.full <- u[county]
M2 <- lmer (y ~ x + u.full + (1 | county))
head(display (M2))

## coef (M2)
## fixef (M2)
## ranef (M2)

## Plots on Figure 12.5
M1 <- lmer (y ~ x + (1 | county))
a.hat.M1 <- fixef(M1)[1] + ranef(M1)$county                
b.hat.M1 <- fixef(M1)[2]

a.hat.M2 <- fixef(M2)[1] + fixef(M2)[3]*u + ranef(M2)$county
b.hat.M2 <- fixef(M2)[2]

x.jitter <- x + runif(n,-.05,.05)
display8 <- c (36, 1, 35, 21, 14, 71, 61, 70)  # counties to be displayed
y.range <- range (y[!is.na(match(county,display8))])

par (mfrow=c(2,4), mar=c(4,4,3,1), oma=c(1,1,2,1))
for (j in display8){
  plot (x.jitter[county==j], y[county==j], xlim=c(-.05,1.05), ylim=y.range,
    xlab="floor", ylab="log radon level", cex.lab=1.2, cex.axis=1.1,
    pch=20, mgp=c(2,.7,0), xaxt="n", yaxt="n", cex.main=1.1, main=uniq[j])
  axis (1, c(0,1), mgp=c(2,.7,0), cex.axis=1.1)
  axis (2, seq(-1,3,2), mgp=c(2,.7,0), cex.axis=1.1)
  curve (a.hat.M1[j,] + b.hat.M1*x, lwd=.5, col="gray10", add=TRUE)
  curve (a.hat.M2[j,] + b.hat.M2*x, lwd=1, col="black", add=TRUE)
}

# Plot of ests & se's vs. county uranium (Figure 12.6)
a.se.M2 <- se.coef(M2)$county

par (mar=c(5,5,4,2)+.1)
plot (u, t(a.hat.M2), cex.lab=1.2, cex.axis=1.1,
      xlab="county-level uranium measure", ylab="est. regression intercept", pch=20,
      ylim=c(0.5,2.0), yaxt="n", xaxt="n", mgp=c(3.5,1.2,0))
axis (1, seq(-1,1,.5), cex.axis=1.1, mgp=c(3.5,1.2,0))
axis (2, cex.axis=1.1, mgp=c(3.5,1.2,0))
curve (fixef(M2)["(Intercept)"] + fixef(M2)["u.full"]*x, lwd=1, col="black", add=TRUE)
for (j in 1:J){
  lines (rep(u[j],2), a.hat.M2[j,] + c(-1,1)*a.se.M2[j,], lwd=.5, col="gray10")
}

### 12.8 Predictions for new observations

```{r sect12point8}
## Prediction for a new observation in a new group (new house in county 26
## with x=1)
M2 <- lmer (y ~ x + u.full + (1 | county))
a.hat.M2 <- fixef(M2)[1] + fixef(M2)[3]*u + ranef(M2)$county
b.hat.M2 <- fixef(M2)[2]

x.tilde <- 1
sigma.y.hat <- sigma.hat(M2)$sigma$data
coef.hat <- as.matrix (coef(M2)$county)[26,]
y.tilde <- rnorm (1, coef.hat %*% c(1, x.tilde, u[26]), sigma.y.hat)
n.sims <- 1000
y.tilde <- rnorm (n.sims, coef.hat %*% c(1, x.tilde, u[26]), sigma.y.hat)

quantile (y.tilde, c(.25, .5, .75))

unlogged <- exp(y.tilde)
mean(unlogged)

## Prediction for a new observation in an existing group (new house in
## a new county)
u.tilde <- mean (u)
g.0.hat <- fixef(M2)["(Intercept)"]
g.1.hat <- fixef(M2)["u.full"]
sigma.a.hat <- sigma.hat(M2)$sigma$county

a.tilde <- rnorm (n.sims, g.0.hat + g.1.hat*u.tilde, sigma.a.hat)
y.tilde <- rnorm (n.sims, a.tilde + b.hat.M2*x.tilde, sigma.y.hat)

quantile (y.tilde, c(.25,.5,.75))

exp (quantile (y.tilde, c(.25,.5,.75)))

## Nonlinear predictions
y.tilde.basement <- rnorm (n.sims, a.hat.M2[26,], sigma.y.hat)
## print (y.tilde.basement)

y.tilde.nobasement <- rnorm (n.sims, a.hat.M2[26,] + b.hat.M2, sigma.y.hat)
## print (y.tilde.nobasement)

mean.radon.basement <- mean (exp (y.tilde.basement))
print (mean.radon.basement)

mean.radon.nobasement <- mean (exp (y.tilde.nobasement))
print (mean.radon.nobasement)

mean.radon <- .9*mean.radon.basement + .1*mean.radon.basement
print (mean.radon)

```

### 12.9 How many group and how many observations per group are needed to fit a multilevel model?

## Closing

### 11.5 Costs and benefits of multilevel modeling

#### Overview of single-level (classical) regression

* Prediction for continuous and discrete outcomes.
* Fitting of nonlinear relations using transformation.
* Inclusion of categorical predictors using indicator variables.
* Modeling of interactions between inputs.

#### Motivations for multilevel modeling

* Accounts for individual and group level variation in estimating *group-level* regression coefficients.
* Modeling variation amoung *individual-level* regression coefficients.
* Estimating regression coefficients for *particular* groups.

#### Costs

* Multilevel models are complex, making specifying and interpreting more of a challenge.
* Increased statistical assumptions, which can result in better inferences or may not be realistic.
