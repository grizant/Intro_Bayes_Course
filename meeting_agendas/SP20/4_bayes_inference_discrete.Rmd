---
title: "Bayesian inference for discrete random variables"
author: "AG Schissler"
date: "02/03/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Start-of-class work:

Let's collect some data to estimate the proportion of students who read Chapter 6 prior to class. No judgements, please answer honestly. To do this, in groups of 3 or 4, collect the number of 1's (did read) and 0's (did not read). Based on these small sample data, what is your estimate of the proportion of students in the class that read?

## I. Bayesian inference for discrete random variables

Discuss the introductory section in Ch.6. Show Table 6.1, 6.2, 6.3 on projector.

Discuss notational change:

$f(x_i, y_j) = g(x_i) \times f(y_j | x_i)$.

Why $g(\cdot)$ and $f(\cdot)$?

Bayes` theorem (again):

$f(x_i | y_j) = \frac{g(x_i) \times f(y_j | x_i)}{\sum_{i=1}^n g(x_i) \times f(y_j | x_i)}$.

Let's label and discuss the parts of the above formula.

### Activity break: Example 6.1

Review Example 6.1, p. 112 - 114. Discuss the steps for Bayes' Theorem using a table with your near peers. Then a volunteer will explain to the class.

## II. Using the `Bolstad` R package

### Demonstration: one at time or all at once?

Imagine we observe 8 independent trials of a random experiment and each has one of two possible outcomes (success = 1 and failure = 0). We'd like to infer what the (unobservable) probability of success is, given the data. Suppose we observe the sequence:

11011101

Let's use the `Bolstad` R package to do this --- first one observation at a time, then sequentially.

First let's install and load the package:

```{r}
## install.packages("Bolstad")
library(Bolstad)

## help(package = "Bolstad")

## Ch.6, Exercise C1
## for help
## ?binodp
```
Now let's revise our belief about the probability of success, $\pi$, by computing Bayes' rule one observation at a time.

```{r, fig.height = 10}
## the observed values
observed_seq <- c(1,1,0,1,1,1,0,1)

## uniform discrete prior, using a grid of 10 values
my_pi <- seq(0, 1, length.out = 10)
my_pi.prior <- rep( 1 / length(my_pi), length(my_pi) )

par(mfrow = c(4,2))
## tmp_obs <- 1
for (tmp_obs in observed_seq) {
    ## compute Bayes rule
    tmp_result <- binodp(x = tmp_obs, n = 1, pi = my_pi, pi.prior = my_pi.prior, suppressOutput = TRUE)
    ## update beliefs
    ## i.e., the posterior becomes the prior
    my_pi.prior <- tmp_result$posterior
}
```

Now let's compute this all at once:

```{r}
par(mfrow = c(1,1))
binodp(x = 6, n = 8)
```

Does our inference about $\pi$ change?

## III. Two equivalent ways of using Bayes' Theorem

Discuss Table 6.7 - 6.10 and notational challenges.

## IV. Using Bayes' rule for binomial and Poisson rvs with discrete prior

Discuss example 6.2 and 6.3, briefly.

### Let's revisit the start of class data and analyze in R.

```{r, echo=FALSE, eval=FALSE}
Bolstad::binodp(x = 12, n = 33)
```

## V. Midterm One discussion

You have the entire 75 minutes. I know it seems early to have "midterm", but in my experience early exposure to evaluation/assessment leads to better performance later and more clarity in expectations. Please attempt HW 2 and 3 before the exam and ask questions either by email or in office hours. Focus on terminology, notation, broad concepts, and basic probabilistic/statistical computation.

### Midterm scope

Content from chapters 4, 5, 6 will be on Midterm One. From the course assessment plan, Midterm One is worth 50 points. The exam evaluates student mastery on the three first learning outcomes:

- recall the axioms, basic terms/algebra of probability, including Bayes' Theorem. (10 points)
- model parameters and data using discrete and continuous random variables. (30 points)
- conduct Bayesian inference for parameters of discrete and continuous random variables. (10 points)

From the point breakdown, you can see the emphasis is on Chapter 5 (applying probability models for discrete random variables).

### Question format

The question format relates to the action verb in the learning outcome. So I'll likely use matching or fill-in-the-blank to make sure you can recall the rules of probability and terms for the first learning outcome. Then I present a series of situations for you to model, calculate probabilities, and interpret (in a similar style as the HW exercises) to assess the second learning outcome. Finally, I'll ask some open-ended questions about the concept of Bayesian inference to check that you've been reading and understanding our discussion.

### What to bring

Scratch paper, handheld calculator, pencils with eraser, 1 page of notes, positive attitude ;)

## Closing

- Brainstorm what to include on your page of notes.
- Discuss with near peers.
- Share ideas with the class.
